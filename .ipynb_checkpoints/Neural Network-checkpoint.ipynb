{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = csv.reader(open(\"hepatitis_2_csv.csv\", \"r\"))\n",
    "x = list(reader)\n",
    "res = np.array(x).astype(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "568\n",
      "(19, 568)\n"
     ]
    }
   ],
   "source": [
    "training_class_name=res[0:568,19:20];\n",
    "training_data=res[0:568,0:19];\n",
    "training_data=np.matrix(training_data)\n",
    "training_data=training_data.transpose()\n",
    "training_class_name=np.matrix(training_class_name)\n",
    "training_class_name=training_class_name.transpose()\n",
    "print(np.prod(list(training_class_name[0].shape)))\n",
    "print(training_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanData=training_data.mean()\n",
    "varianceData=np.var(training_data)\n",
    "training_data=(training_data-meanData)/varianceData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net:\n",
    "    def __init__(self):\n",
    "        self.weights=[];\n",
    "        self.bias=[];\n",
    "        self.status=[];\n",
    "        self.a=[[[]]];\n",
    "        self.z=[[[]]];\n",
    "        self.da=[[[]]];\n",
    "        self.dz=[[[]]];\n",
    "        self.dw=[[[]]];\n",
    "        self.db=[[[]]];\n",
    "        self.cost=[];\n",
    "        self.trainingOutput=[[[]]];\n",
    "        self.correctOutput=[];\n",
    "        self.size=0;\n",
    "        self.vw=[];\n",
    "        self.vb=[];\n",
    "        self.sw=[];\n",
    "        self.sb=[];\n",
    "        self.vwcorrected=[];\n",
    "        self.vbcorrected=[];\n",
    "        self.swcorrected=[];\n",
    "        self.sbcorrected=[];\n",
    "        self.dwforgrad=[];\n",
    "        self.dbforgrad=[]\n",
    "        \n",
    "    def model(self,layers):\n",
    "        self.layers=layers\n",
    "        self.status.append(\"Model archetecture recieved\")\n",
    "        self.createBrain()\n",
    "        self.status.append(\"Model archetecture created\")\n",
    "        return self.layers\n",
    "    \n",
    "    def createBrain(self):\n",
    "        inputshape = self.layers[0].getshape()\n",
    "        self.weights.append(0.01*np.random.rand(self.layers[1].getparams()[\"noOfUnits\"],inputshape[0]))\n",
    "        self.bias.append(np.ones([self.layers[1].getparams()[\"noOfUnits\"],inputshape[1]]))\n",
    "        print(\"layer 1 has bias shape as {} and weights shape as {}\".format(self.bias[0].shape,self.weights[0].shape))\n",
    "        for i in range(2,len(self.layers)):\n",
    "            lowerlimit=self.layers[i].getparams()[\"noOfUnits\"]\n",
    "            upperlimit=self.layers[i-1].getparams()[\"noOfUnits\"]\n",
    "            self.weights.append(0.01*np.random.rand(lowerlimit,upperlimit))\n",
    "            self.bias.append(np.ones([lowerlimit,inputshape[1]]))\n",
    "            print(\"layer {} has bias shape as {} and weights shape as {}\".format(i,self.bias[i-1].shape,self.weights[i-1].shape))\n",
    "    \n",
    "    def train(self,traindata,trainoutcome,hyperparams):\n",
    "        \n",
    "        numberOfExamples = np.prod(list(training_class_name[0].shape))   \n",
    "        epochs=hyperparams[\"epochs\"]\n",
    "        self.epochs=np.asmatrix(np.arange(0,epochs,1))\n",
    "        democost=0\n",
    "        \n",
    "        if(hyperparams[\"Batch_size\"]>0):\n",
    "            minibatch=True\n",
    "            size=hyperparams[\"Batch_size\"]\n",
    "        \n",
    "            if(minibatch):\n",
    "                self.bias[0]=np.ones([self.layers[1].getparams()[\"noOfUnits\"],size])\n",
    "                for i in range(1,len(self.layers)):\n",
    "                    lowerlimit=self.layers[i].getparams()[\"noOfUnits\"]\n",
    "                    self.bias[i-1]=np.ones([lowerlimit,size])\n",
    "\n",
    "                minibatches = self.make_mini_batches(numberOfExamples,traindata,trainoutcome,30)\n",
    "        else:\n",
    "            minibatches = [[(traindata),(trainoutcome)]]\n",
    "            \n",
    "        for i in range(0,epochs):\n",
    "\n",
    "            \n",
    "            for j in range(0,len(minibatches)):                              \n",
    "                # Select a minibatch\n",
    "                (traindata, trainclass) = minibatches[j]\n",
    "                self.size = traindata.shape[1] \n",
    "                op = self.forewardProp(numberOfExamples,traindata,trainclass,i,j)\n",
    "                val = op[\"val\"]\n",
    "                               \n",
    "                if(self.layers[1].getparams()[\"activation\"]==\"sigmoid\"):\n",
    "                    val=val>=0.499\n",
    "                elif(self.layers[1].getparams()[\"activation\"]==\"tanh\" or self.layers[1].getparams()[\"activation\"]==\"relu\"):\n",
    "                    val=val>=0\n",
    "                 \n",
    "                democost=democost+op[\"cost\"]\n",
    "                \n",
    "                self.trainingOutput[i][j].append(val)\n",
    "                self.correctOutput[i][j].append(trainclass)\n",
    "                self.status.append(\"Foreward Propogation for batch {} in epoch {} is completed\".format(j,i))\n",
    "                self.backwardProp(numberOfExamples,trainclass,i,j,hyperparams)\n",
    "                self.status.append(\"Backward Propogation for batch {} in epoch {} is completed\".format(j,i))\n",
    "                self.da[i].append([])\n",
    "                self.a[i].append([])\n",
    "                self.z[i].append([])\n",
    "                self.dz[i].append([])\n",
    "                self.dw[i].append([])\n",
    "                self.db[i].append([])\n",
    "                self.trainingOutput[i].append([])\n",
    "                self.correctOutput.append([])\n",
    "                \n",
    "            self.cost.append(democost) \n",
    "            print(\"cost for epoch {} is {}\".format(i,democost))  \n",
    "            democost=0\n",
    "            self.status.append(\"Foreward Propogation for epoch {} is completed\".format(i))\n",
    "            self.status.append(\"Backward Propogation for epoch {} is completed\".format(i))                  \n",
    "            self.da.append([[]])\n",
    "            self.dz.append([[]])\n",
    "            self.dw.append([[]])\n",
    "            self.db.append([[]])\n",
    "            self.a.append([[]])\n",
    "            self.z.append([])\n",
    "            self.trainingOutput.append([])\n",
    "            self.correctOutput.append([])\n",
    "            \n",
    "        #self.trainingOutput=np.asarray(self.trainingOutput)\n",
    "        #self.correctOutput=np.asarray(self.correctOutput)\n",
    "        #self.dz=np.asarray(self.dz)\n",
    "        #self.dw=np.asarray(self.dw)\n",
    "        #self.db=np.asarray(self.db)\n",
    "        #self.da=np.asarray(self.da)\n",
    "\n",
    "    \n",
    "    def forewardProp(self,m,traindata,trainoutcome,iterno,batch):\n",
    "        self.a[iterno][batch].append(traindata)\n",
    "        z=self.weights[0]*traindata+self.bias[0][:,np.arange(0,self.size)]\n",
    "        self.z[iterno][batch].append(z)\n",
    "        \n",
    "        if(self.layers[1].getparams()[\"activation\"]==\"sigmoid\"):\n",
    "            aval=self.sigmoid(z)\n",
    "            self.a[iterno][batch].append(aval)\n",
    "        elif(self.layers[1].getparams()[\"activation\"]==\"tanh\"):\n",
    "            aval=self.tanh(z)\n",
    "            self.a[iterno][batch].append(aval)\n",
    "        elif(self.layers[1].getparams()[\"activation\"]==\"relu\"):\n",
    "            aval=self.relu(z)\n",
    "            self.a[iterno][batch].append(aval)       \n",
    "        \n",
    "        for i in range(2,len(self.layers)):\n",
    "            #print(self.weights[i-1].shape,self.a[iterno][batch][i-1].shape,self.bias[i-1].shape)\n",
    "            z=(self.weights[i-1]*self.a[iterno][batch][i-1])+self.bias[i-1][:,np.arange(0,self.size)]\n",
    "            self.z[iterno][batch].append(z)\n",
    "            #print(z)\n",
    "            if(self.layers[1].getparams()[\"activation\"]==\"sigmoid\"):\n",
    "                aval=self.sigmoid(z)\n",
    "                self.a[iterno][batch].append(aval)\n",
    "            elif(self.layers[1].getparams()[\"activation\"]==\"tanh\"):\n",
    "                aval=self.tanh(z)\n",
    "                self.a[iterno][batch].append(aval)\n",
    "            elif(self.layers[1].getparams()[\"activation\"]==\"relu\"):\n",
    "                aval=self.relu(z)\n",
    "                self.a[iterno][batch].append(aval)\n",
    "        \n",
    "        cost=self.costfunc(m,traindata,trainoutcome,self.a[iterno][batch][i])   \n",
    "        returnval = {\"val\":self.a[iterno][batch][len(self.layers)-1],\"cost\":cost}\n",
    "            \n",
    "        return returnval\n",
    "     \n",
    "    def costfunc(self,m,traindata,trainoutcome,res):\n",
    "        #print(1-res)\n",
    "        A1=np.multiply(trainoutcome,np.log(res))\n",
    "        A2=np.multiply((1-trainoutcome),np.log((1-res)+0.99999))\n",
    "        cost=A1+A2\n",
    "        cost=np.sum(cost)\n",
    "        cost=(-1/m)*cost\n",
    "        return cost\n",
    "    \n",
    "    def backwardProp(self,m,trainoutcome,iterno,batch,hyperparams):\n",
    "        self.optimizationinitialize()\n",
    "        AL=self.a[iterno][batch][len(self.layers)-1]\n",
    "        dAL= - ( np.divide(trainoutcome,AL) - np.divide(1-trainoutcome,1-AL) )\n",
    "        self.da[iterno][batch].append(dAL);\n",
    "        for i in range(0,len(self.layers)-1): \n",
    "            \n",
    "            if(self.layers[1].getparams()[\"activation\"]==\"sigmoid\"):\n",
    "                prime=self.sigmoid_Prime(self.z[iterno][batch][len(self.layers)-2-i])\n",
    "            elif(self.layers[1].getparams()[\"activation\"]==\"tanh\"):\n",
    "                prime=self.tanh_Prime(self.z[iterno][batch][len(self.layers)-2-i])\n",
    "            elif(self.layers[1].getparams()[\"activation\"]==\"relu\"):\n",
    "                prime=self.relu_Prime(self.z[iterno][batch][len(self.layers)-2-i])\n",
    "                \n",
    "            dz = np.multiply(dAL,prime)\n",
    "            self.dz[iterno][batch].append(dz)\n",
    "            dw = (1/self.size) * ( dz * ( self.a[iterno][batch][len(self.layers)-2-i].transpose() ) )\n",
    "            self.dw[iterno][batch].append(dw)\n",
    "            db = (1/self.size)*np.sum(dz,axis=1)\n",
    "            self.db[iterno][batch].append(db)\n",
    "            dAL=(self.weights[len(self.layers)-2-i].transpose())*dz\n",
    "            self.da[iterno][batch].append(dAL)\n",
    "            \n",
    "            # Updating weights\n",
    "            \n",
    "            if(hyperparams[\"optimizer\"]==\"momentum\"):\n",
    "                self.updateMomentum(hyperparams[\"beta\"],hyperparams[\"LearningRate\"],dw,db,i)\n",
    "            elif(hyperparams[\"optimizer\"]==\"adam\"):\n",
    "                self.updateAdam(hyperparams[\"beta1\"],hyperparams[\"beta2\"],hyperparams[\"LearningRate\"],dw,db,i,batch+1,hyperparams[\"epsilon\"])\n",
    "            elif(hyperparams[\"optimizer\"]==\"gd\"):\n",
    "                self.updateGD(hyperparams[\"LearningRate\"],dw,db,i)\n",
    "            \n",
    "    def optimizationinitialize(self):\n",
    "        for i in range(0,len(self.layers)-1):\n",
    "            self.vw.append(np.zeros(list(self.weights[i].shape)))\n",
    "            self.vb.append(np.zeros(list(self.bias[i].shape)))\n",
    "            self.sw.append(np.zeros(list(self.weights[i].shape)))\n",
    "            self.sb.append(np.zeros(list(self.bias[i].shape)))\n",
    "            self.vwcorrected.append(np.zeros(list(self.weights[i].shape)))\n",
    "            self.vbcorrected.append(np.zeros(list(self.bias[i].shape)))\n",
    "            self.swcorrected.append(np.zeros(list(self.weights[i].shape)))\n",
    "            self.sbcorrected.append(np.zeros(list(self.bias[i].shape)))\n",
    "            self.dwforgrad.append(np.zeros(list(self.bias[i].shape)))\n",
    "            self.dbforgrad.append(np.zeros(list(self.bias[i].shape)))\n",
    "            \n",
    "        \n",
    "    def updateGD(self,learningRate,dw,db,i):\n",
    "        self.dwforgrad[len(self.layers)-2-i] = dw\n",
    "        self.dbforgrad[len(self.layers)-2-i] = db\n",
    "        \n",
    "        self.weights[len(self.layers)-2-i] = self.weights[len(self.layers)-2-i] - learningRate*dw\n",
    "        self.bias[len(self.layers)-2-i] = self.bias[len(self.layers)-2-i] - learningRate*db\n",
    "        \n",
    "    def updateMomentum(self,beta,learningRate,dw,db,i):\n",
    "        self.dwforgrad[len(self.layers)-2-i] = dw\n",
    "        self.dbforgrad[len(self.layers)-2-i] = db\n",
    "        \n",
    "        self.vw[len(self.layers)-2-i] = beta*self.vw[len(self.layers)-2-i] + (1-beta)*dw\n",
    "        self.vb[len(self.layers)-2-i] = beta*self.vb[len(self.layers)-2-i] + (1-beta)*db\n",
    "        \n",
    "        self.weights[len(self.layers)-2-i] = self.weights[len(self.layers)-2-i] - learningRate*self.vw[len(self.layers)-2-i]\n",
    "        self.bias[len(self.layers)-2-i] = self.bias[len(self.layers)-2-i] - learningRate*self.vb[len(self.layers)-2-i]\n",
    "    \n",
    "    def updateAdam(self,beta1,beta2,learningRate,dw,db,i,batch,epsilon):\n",
    "        self.dwforgrad[len(self.layers)-2-i] = dw\n",
    "        self.dbforgrad[len(self.layers)-2-i] = db\n",
    "        \n",
    "        self.vw[len(self.layers)-2-i] = beta1*self.vw[len(self.layers)-2-i] + (1-beta1)*dw\n",
    "        self.vb[len(self.layers)-2-i] = beta1*self.vb[len(self.layers)-2-i] + (1-beta1)*db\n",
    "        \n",
    "        vw_corrected = self.vw[len(self.layers)-2-i] / (1 - np.power(beta1,batch) )\n",
    "        vb_corrected = self.vb[len(self.layers)-2-i] / (1 - np.power(beta1,batch) )\n",
    "        \n",
    "        self.vwcorrected[len(self.layers)-2-i]=vw_corrected\n",
    "        self.vbcorrected[len(self.layers)-2-i]=vb_corrected\n",
    "        \n",
    "        self.sw[len(self.layers)-2-i] = beta2*self.sw[len(self.layers)-2-i] + (1-beta2)*np.power(dw,2)\n",
    "        self.sb[len(self.layers)-2-i] = beta2*self.sb[len(self.layers)-2-i] + (1-beta2)*np.power(db,2)\n",
    "        \n",
    "        sw_corrected = self.sw[len(self.layers)-2-i] / (1 - np.power(beta2,batch) )\n",
    "        sb_corrected = self.sb[len(self.layers)-2-i] / (1 - np.power(beta2,batch) )\n",
    "        \n",
    "        self.swcorrected[len(self.layers)-2-i]=sw_corrected\n",
    "        self.sbcorrected[len(self.layers)-2-i]=sb_corrected\n",
    "        \n",
    "        self.weights[len(self.layers)-2-i] = self.weights[len(self.layers)-2-i] - learningRate * (vw_corrected / np.sqrt(sw_corrected + epsilon))\n",
    "        self.bias[len(self.layers)-2-i] = self.bias[len(self.layers)-2-i] - learningRate * (vb_corrected / np.sqrt(sb_corrected + epsilon))\n",
    "        \n",
    "    \n",
    "    def make_mini_batches(self,m,training_data,training_class_names,batch_Size):\n",
    "        mini_batches=[]\n",
    "        new_training_data=training_data\n",
    "        new_training_class_names=training_class_names\n",
    "        \n",
    "        # Randomize The Training Data\n",
    "        permutation = list(np.random.permutation(m))\n",
    "        new_training_data = training_data[:,permutation]\n",
    "        new_training_class_names=training_class_names[:, permutation].reshape((1,m))\n",
    "        \n",
    "        # Get Number Of Batches\n",
    "        num_minibatches = math.floor(m/batch_Size)\n",
    "        # Split The Training_Data Into Batches\n",
    "        for i in range(0,num_minibatches):\n",
    "            training_batch = new_training_data[:,batch_Size*i:batch_Size *(i+1)]\n",
    "            training_class_batch = new_training_class_names[:,batch_Size*i:batch_Size *(i+1)]\n",
    "            mini_batch = (training_batch, training_class_batch)\n",
    "            mini_batches.append(mini_batch)\n",
    "            \n",
    "        # Last Batch If Exists\n",
    "        if m % batch_Size != 0:\n",
    "            training_batch = new_training_data[:,batch_Size*(i+1):m]\n",
    "            training_class_batch = new_training_class_names[:,batch_Size*(i+1):m]\n",
    "            mini_batch = (training_batch, training_class_batch)\n",
    "            mini_batches.append(mini_batch)\n",
    "        \n",
    "        mini_batches = np.asarray(mini_batches)\n",
    "            \n",
    "        return mini_batches\n",
    "        \n",
    "    def sigmoid_Prime(self,value):\n",
    "        part1=self.sigmoid(value)\n",
    "        part2=1-(self.sigmoid(value))\n",
    "        return np.multiply( part1 , part2 )\n",
    "    \n",
    "    def tanh_Prime(self,value):\n",
    "        return 1-(np.power(self.tanh(value),2))\n",
    "    \n",
    "    def relu_Prime(self,value):\n",
    "        return value>=0\n",
    "        \n",
    "    def results(self):\n",
    "        loss=[]\n",
    "        accuracy=[]\n",
    "        for i in range(0,len(self.trainingOutput)):\n",
    "            print(i)\n",
    "            trainingOutput=np.asarray(self.trainingOutput[i])\n",
    "            correctOutput=np.asarray(self.correctOutput[i])\n",
    "            #print(trainingOutput,correctOutput)\n",
    "            #print(correctOutput.shape,trainingOutput.shape)\n",
    "            ls=correctOutput - trainingOutput\n",
    "            lossval=np.sum(ls)\n",
    "            lossval=lossval/len(ls[0])\n",
    "            loss.append(lossval)\n",
    "            ac=np.sum(correctOutput==trainingOutput)\n",
    "            accuracyval=ac/len(correctOutput[0])\n",
    "            accuracy.append(accuracyval)\n",
    "            #plt.subplot(1,2,1)\n",
    "            #plt.plot(self.epochs.T,accuracy)\n",
    "            #plt.title(\"Training Accuracy\")\n",
    "            #plt.xlabel(\"epochs\")\n",
    "            #plt.ylabel(\"Accuracy\")\n",
    "            #plt.subplot(1,2,2)\n",
    "            #plt.plot(self.epochs.T,loss)\n",
    "            #plt.title(\"Training Loss\")\n",
    "            #plt.xlabel(\"epochs\")\n",
    "            #plt.ylabel(\"Loss\")\n",
    "            #print(\"Accuracy : {}\".format(accuracy[len(accuracy)-1]))\n",
    "            #print(\"loss : {}\".format(loss[len(loss)-1]))\n",
    "        print(\"accuracy = \",accuracy)\n",
    "        print(\"accuracy len = \",len(accuracy))\n",
    "    \n",
    "    def sigmoid(self,X):\n",
    "        return 1/(1+np.exp(-X));\n",
    "\n",
    "    def tanh(self,X):\n",
    "        return np.tanh(X);\n",
    "\n",
    "    def relu(self,X):\n",
    "        return np.maximum(0, X);\n",
    "\n",
    "    def getActivation(self):\n",
    "        # plot the activation function using matplotlib library\n",
    "        cur_axes = plt.gca()\n",
    "        # to remove the x axis\n",
    "        cur_axes.axes.get_xaxis().set_visible(False)\n",
    "        rangex=np.linspace(-10, 10, 100)\n",
    "\n",
    "        if(self.activation==\"sigmoid\"):\n",
    "            plt.plot(rangex,self.sigmoid(rangex))\n",
    "        elif(self.activation==\"tanh\"):\n",
    "            plt.plot(rangex,self.tanh(rangex))\n",
    "        elif(self.activation==\"relu\"):\n",
    "            plt.plot(rangex,self.relu(rangex))\n",
    "            plt.title(\"activation function = \"+self.activation)\n",
    "        \n",
    "    class Layers:\n",
    "        \n",
    "        class InputLayer:\n",
    "            \n",
    "            def __init__(self,shape):\n",
    "                self.shape=shape\n",
    "                \n",
    "            def getshape(self):\n",
    "                return np.asarray(self.shape).transpose()\n",
    "        \n",
    "        class DenseLayer:\n",
    "            \n",
    "            def __init__(self,noOfUnits,activation):\n",
    "                self.params={\"activation\":activation,\"shape\":(noOfUnits,1),\"noOfUnits\":noOfUnits}\n",
    "                \n",
    "            def getparams(self):\n",
    "                return self.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 1 has bias shape as (15, 568) and weights shape as (15, 19)\n",
      "layer 2 has bias shape as (10, 568) and weights shape as (10, 15)\n",
      "layer 3 has bias shape as (5, 568) and weights shape as (5, 10)\n",
      "layer 4 has bias shape as (4, 568) and weights shape as (4, 5)\n",
      "layer 5 has bias shape as (1, 568) and weights shape as (1, 4)\n"
     ]
    }
   ],
   "source": [
    "my_brain = Net()\n",
    "l1=Net.Layers.InputLayer(training_data.shape)\n",
    "l2=Net.Layers.DenseLayer(15 , activation=\"tanh\")\n",
    "l3=Net.Layers.DenseLayer(10 , activation=\"tanh\")\n",
    "l4=Net.Layers.DenseLayer(5 , activation=\"tanh\")\n",
    "l5=Net.Layers.DenseLayer(4 , activation=\"tanh\")\n",
    "l6=Net.Layers.DenseLayer(1 , activation=\"sigmoid\")\n",
    "archetecture = my_brain.model(\n",
    "    layers=[l1,l2,l3,l4,l5,l6]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings={\"LearningRate\":0.1,\"Batch_size\":30,\"optimizer\":\"adam\",\"beta1\":0.9,\"beta2\":0.999,\"epochs\":10,\"epsilon\": 1e-8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost for epoch 0 is 0.1346949646507171\n",
      "cost for epoch 1 is 0.15556946633666807\n",
      "cost for epoch 2 is 0.13281059753492405\n",
      "cost for epoch 3 is 0.14091996812351684\n",
      "cost for epoch 4 is 0.14358852263051905\n",
      "cost for epoch 5 is 0.14251892730048954\n",
      "cost for epoch 6 is 0.13557105014560983\n",
      "cost for epoch 7 is 0.1337625615833811\n",
      "cost for epoch 8 is 0.13331968166762442\n",
      "cost for epoch 9 is 0.1348183355810656\n"
     ]
    }
   ],
   "source": [
    "my_brain.train(training_data,training_class_name,settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = my_brain.make_mini_batches(568,training_data,training_class_name,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28)\n"
     ]
    }
   ],
   "source": [
    "print(result[18][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "accuracy =  [0.8666666666666667, 0.7666666666666667, 0.8, 0.7666666666666667, 0.7666666666666667, 0.6666666666666666, 0.7666666666666667, 0.7666666666666667, 0.8333333333333334, 0.8, 0.8666666666666667, 0.9666666666666667, 0.9, 0.8333333333333334, 0.6666666666666666, 0.8, 0.9333333333333333, 0.8333333333333334, 0.9285714285714286, 0.8666666666666667, 0.7666666666666667, 0.8, 0.7666666666666667, 0.7666666666666667, 0.6666666666666666, 0.7666666666666667, 0.7666666666666667, 0.8333333333333334, 0.8, 0.8666666666666667, 0.9666666666666667, 0.9, 0.8333333333333334, 0.6666666666666666, 0.8, 0.9333333333333333, 0.8333333333333334, 0.9285714285714286, 0.8666666666666667, 0.7666666666666667, 0.8, 0.7666666666666667, 0.7666666666666667, 0.6666666666666666, 0.7666666666666667, 0.7666666666666667, 0.8333333333333334, 0.8, 0.8666666666666667, 0.9666666666666667, 0.9, 0.8333333333333334, 0.6666666666666666, 0.8, 0.9333333333333333, 0.8333333333333334, 0.9285714285714286, 0.8666666666666667, 0.7666666666666667, 0.8, 0.7666666666666667, 0.7666666666666667, 0.6666666666666666, 0.7666666666666667, 0.7666666666666667, 0.8333333333333334, 0.8, 0.8666666666666667, 0.9666666666666667, 0.9, 0.8333333333333334, 0.6666666666666666, 0.8, 0.9333333333333333, 0.8333333333333334, 0.9285714285714286, 0.8666666666666667, 0.7666666666666667, 0.8, 0.7666666666666667, 0.7666666666666667, 0.6666666666666666, 0.7666666666666667, 0.7666666666666667, 0.8333333333333334, 0.8, 0.8666666666666667, 0.9666666666666667, 0.9, 0.8333333333333334, 0.6666666666666666, 0.8, 0.9333333333333333, 0.8333333333333334, 0.9285714285714286, 0.8666666666666667, 0.7666666666666667, 0.8, 0.7666666666666667, 0.7666666666666667, 0.6666666666666666, 0.7666666666666667, 0.7666666666666667, 0.8333333333333334, 0.8, 0.8666666666666667, 0.9666666666666667, 0.9, 0.8333333333333334, 0.6666666666666666, 0.8, 0.9333333333333333, 0.8333333333333334, 0.9285714285714286, 0.8666666666666667, 0.7666666666666667, 0.8, 0.7666666666666667, 0.7666666666666667, 0.6666666666666666, 0.7666666666666667, 0.7666666666666667, 0.8333333333333334, 0.8, 0.8666666666666667, 0.9666666666666667, 0.9, 0.8333333333333334, 0.6666666666666666, 0.8, 0.9333333333333333, 0.8333333333333334, 0.9285714285714286, 0.8666666666666667, 0.7666666666666667, 0.8, 0.7666666666666667, 0.7666666666666667, 0.6666666666666666, 0.7666666666666667, 0.7666666666666667, 0.8333333333333334, 0.8, 0.8666666666666667, 0.9666666666666667, 0.9, 0.8333333333333334, 0.6666666666666666, 0.8, 0.9333333333333333, 0.8333333333333334, 0.9285714285714286, 0.8666666666666667, 0.7666666666666667, 0.8, 0.7666666666666667, 0.7666666666666667, 0.6666666666666666, 0.7666666666666667, 0.7666666666666667, 0.8333333333333334, 0.8, 0.8666666666666667, 0.9666666666666667, 0.9, 0.8333333333333334, 0.6666666666666666, 0.8, 0.9333333333333333, 0.8333333333333334, 0.9285714285714286, 0.8666666666666667, 0.7666666666666667, 0.8, 0.7666666666666667, 0.7666666666666667, 0.6666666666666666, 0.7666666666666667, 0.7666666666666667, 0.8333333333333334, 0.8, 0.8666666666666667, 0.9666666666666667, 0.9, 0.8333333333333334, 0.6666666666666666, 0.8, 0.9333333333333333, 0.8333333333333334, 0.9285714285714286]\n",
      "accuracy len =  190\n"
     ]
    }
   ],
   "source": [
    "results=my_brain.results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4  5]\n",
      " [ 4  5  6  7  8]\n",
      " [ 7  8  9  1  2]\n",
      " [11 21 31 41 15]\n",
      " [ 4  5  6  7  8]\n",
      " [ 7  8  9  1  2]\n",
      " [12 22 32 42 52]\n",
      " [ 4  5  6  7  8]\n",
      " [ 7  8  9  1  2]]\n"
     ]
    }
   ],
   "source": [
    "a=np.array([[1,2,3,4,5],[4,5,6,7,8],[7,8,9,1,2],[11,21,31,41,15],[4,5,6,7,8],[7,8,9,1,2],[12,22,32,42,52],[4,5,6,7,8],[7,8,9,1,2]])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 7, 3, 5, 8, 1, 6, 4, 2]\n",
      "[[ 1  2  3  4  5]\n",
      " [ 4  5  6  7  8]\n",
      " [11 21 31 41 15]\n",
      " [ 7  8  9  1  2]\n",
      " [ 7  8  9  1  2]\n",
      " [ 4  5  6  7  8]\n",
      " [12 22 32 42 52]\n",
      " [ 4  5  6  7  8]\n",
      " [ 7  8  9  1  2]]\n"
     ]
    }
   ],
   "source": [
    "permutation = list(np.random.permutation(len(a)))\n",
    "print(permutation)\n",
    "new_a = a[permutation,:]\n",
    "print(new_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc=[[[[0],[1],[2]]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(abc[0][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(0,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.floor(560/30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0000050000287824e-05\n"
     ]
    }
   ],
   "source": [
    "print(np.log(0.99999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
