{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = csv.reader(open(\"hepatitis_2_csv.csv\", \"r\"))\n",
    "x = list(reader)\n",
    "res = np.array(x).astype(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "568\n",
      "(19, 568)\n"
     ]
    }
   ],
   "source": [
    "training_class_name=res[0:568,19:20];\n",
    "training_data=res[0:568,0:19];\n",
    "training_data=np.matrix(training_data)\n",
    "training_data=training_data.transpose()\n",
    "training_class_name=np.matrix(training_class_name)\n",
    "training_class_name=training_class_name.transpose()\n",
    "print(np.prod(list(training_class_name[0].shape)))\n",
    "print(training_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanData=training_data.mean()\n",
    "varianceData=np.var(training_data)\n",
    "training_data=(training_data-meanData)/varianceData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net:\n",
    "    def __init__(self):\n",
    "        self.weights=[];\n",
    "        self.bias=[];\n",
    "        self.status=[];\n",
    "        self.a=[[[]]];\n",
    "        self.z=[[[]]];\n",
    "        self.da=[[[]]];\n",
    "        self.dz=[[[]]];\n",
    "        self.dw=[[[]]];\n",
    "        self.db=[[[]]];\n",
    "        self.cost=[];\n",
    "        self.trainingOutput=[[[]]];\n",
    "        self.correctOutput=[[[]]];\n",
    "        self.size=0;\n",
    "        self.vw=[];\n",
    "        self.vb=[];\n",
    "        self.sw=[];\n",
    "        self.sb=[];\n",
    "        self.vwcorrected=[];\n",
    "        self.vbcorrected=[];\n",
    "        self.swcorrected=[];\n",
    "        self.sbcorrected=[];\n",
    "        self.dwforgrad=[];\n",
    "        self.dbforgrad=[];\n",
    "        \n",
    "    def model(self,layers):\n",
    "        self.layers=layers\n",
    "        self.status.append(\"Model archetecture recieved\")\n",
    "        self.createBrain()\n",
    "        self.status.append(\"Model archetecture created\")\n",
    "        return self.layers\n",
    "    \n",
    "    def createBrain(self):\n",
    "        inputshape = self.layers[0].getshape()\n",
    "        self.weights.append(0.01*np.random.rand(self.layers[1].getparams()[\"noOfUnits\"],inputshape[0]))\n",
    "        self.bias.append(np.ones([self.layers[1].getparams()[\"noOfUnits\"],inputshape[1]]))\n",
    "        print(\"layer 1 has bias shape as {} and weights shape as {}\".format(self.bias[0].shape,self.weights[0].shape))\n",
    "        for i in range(2,len(self.layers)):\n",
    "            lowerlimit=self.layers[i].getparams()[\"noOfUnits\"]\n",
    "            upperlimit=self.layers[i-1].getparams()[\"noOfUnits\"]\n",
    "            self.weights.append(0.01*np.random.rand(lowerlimit,upperlimit))\n",
    "            self.bias.append(np.ones([lowerlimit,inputshape[1]]))\n",
    "            print(\"layer {} has bias shape as {} and weights shape as {}\".format(i,self.bias[i-1].shape,self.weights[i-1].shape))\n",
    "    \n",
    "    def train(self,traindata,trainoutcome,hyperparams):\n",
    "        \n",
    "        numberOfExamples = np.prod(list(training_class_name[0].shape))   \n",
    "        epochs=hyperparams[\"epochs\"]\n",
    "        self.epochs=np.asmatrix(np.arange(0,epochs,1))\n",
    "        democost=0\n",
    "        \n",
    "        if(hyperparams[\"Batch_size\"]>0):\n",
    "            minibatch=True\n",
    "            size=hyperparams[\"Batch_size\"]\n",
    "        \n",
    "            if(minibatch):\n",
    "                self.bias[0]=np.ones([self.layers[1].getparams()[\"noOfUnits\"],size])\n",
    "                for i in range(1,len(self.layers)):\n",
    "                    lowerlimit=self.layers[i].getparams()[\"noOfUnits\"]\n",
    "                    self.bias[i-1]=np.ones([lowerlimit,size])\n",
    "\n",
    "                minibatches = self.make_mini_batches(numberOfExamples,traindata,trainoutcome,30)\n",
    "        else:\n",
    "            minibatches = [[(traindata),(trainoutcome)]]\n",
    "            \n",
    "        # initialize loss and accuracy lists\n",
    "        self.loss = np.zeros([1, epochs])\n",
    "        self.accuracy = np.zeros([1, epochs])\n",
    "        lv=0;\n",
    "        av=0;\n",
    "            \n",
    "        for i in range(0,epochs):\n",
    "\n",
    "            \n",
    "            for j in range(0,len(minibatches)):                              \n",
    "                # Select a minibatch\n",
    "                (traindata, trainclass) = minibatches[j]\n",
    "                self.size = traindata.shape[1] \n",
    "                op = self.forewardProp(numberOfExamples,traindata,trainclass,i,j)\n",
    "                val = op[\"val\"]\n",
    "                               \n",
    "                if(self.layers[1].getparams()[\"activation\"]==\"sigmoid\"):\n",
    "                    val=val>=0.499\n",
    "                elif(self.layers[1].getparams()[\"activation\"]==\"tanh\" or self.layers[1].getparams()[\"activation\"]==\"relu\"):\n",
    "                    val=val>=0\n",
    "                 \n",
    "                democost=democost+op[\"cost\"]\n",
    "                \n",
    "                self.trainingOutput[i][j].append(val)\n",
    "                self.correctOutput[i][j].append(trainclass)\n",
    "                self.status.append(\"Foreward Propogation for batch {} in epoch {} is completed\".format(j,i))\n",
    "                self.backwardProp(numberOfExamples,trainclass,i,j,hyperparams)\n",
    "                self.status.append(\"Backward Propogation for batch {} in epoch {} is completed\".format(j,i))\n",
    "                self.da[i].append([])\n",
    "                self.a[i].append([])\n",
    "                self.z[i].append([])\n",
    "                self.dz[i].append([])\n",
    "                self.dw[i].append([])\n",
    "                self.db[i].append([])\n",
    "                self.trainingOutput[i].append([])\n",
    "                self.correctOutput[i].append([])\n",
    "                # compute loss\n",
    "                ls = trainclass - val\n",
    "                lossval = np.sum(ls)\n",
    "                lossval=lossval/len(ls[0])\n",
    "                lv = lv + lossval \n",
    "                # compute accuracy\n",
    "                ac = np.sum(trainclass==val)\n",
    "                #print(trainclass==val)\n",
    "                #print(ac)\n",
    "                accuracyval = ac/len(trainclass)\n",
    "                av = av + accuracyval\n",
    "\n",
    "                \n",
    "            self.cost.append(democost) \n",
    "            print(\"cost for epoch {} is {}\".format(i,democost))  \n",
    "            democost=0\n",
    "            self.status.append(\"Foreward Propogation for epoch {} is completed\".format(i))\n",
    "            self.status.append(\"Backward Propogation for epoch {} is completed\".format(i))                  \n",
    "            self.da.append([[]])\n",
    "            self.dz.append([[]])\n",
    "            self.dw.append([[]])\n",
    "            self.db.append([[]])\n",
    "            self.a.append([[]])\n",
    "            self.z.append([[]])\n",
    "            self.trainingOutput.append([[]])\n",
    "            self.correctOutput.append([[]])\n",
    "            #print(i)\n",
    "            #print(\"loss = {} accuracy = {}\".format(lv/len(minibatches),av/len(minibatches)))\n",
    "            self.loss[0][i] = lv/len(minibatches)\n",
    "            self.accuracy[0][i] = av/len(minibatches)\n",
    "            #print(\"self.loss = {} and self.accuracy = {}\".format(self.loss[0][i],self.accuracy[0][i]))\n",
    "            lv = 0;\n",
    "            av = 0;\n",
    "            \n",
    "            \n",
    "        #self.trainingOutput=np.asarray(self.trainingOutput)\n",
    "        #self.correctOutput=np.asarray(self.correctOutput)\n",
    "        #self.dz=np.asarray(self.dz)\n",
    "        #self.dw=np.asarray(self.dw)\n",
    "        #self.db=np.asarray(self.db)\n",
    "        #self.da=np.asarray(self.da)\n",
    "\n",
    "    \n",
    "    def forewardProp(self,m,traindata,trainoutcome,iterno,batch):\n",
    "        self.a[iterno][batch].append(traindata)\n",
    "        z=self.weights[0]*traindata+self.bias[0][:,np.arange(0,self.size)]\n",
    "        self.z[iterno][batch].append(z)\n",
    "        \n",
    "        if(self.layers[1].getparams()[\"activation\"]==\"sigmoid\"):\n",
    "            aval=self.sigmoid(z)\n",
    "            self.a[iterno][batch].append(aval)\n",
    "        elif(self.layers[1].getparams()[\"activation\"]==\"tanh\"):\n",
    "            aval=self.tanh(z)\n",
    "            self.a[iterno][batch].append(aval)\n",
    "        elif(self.layers[1].getparams()[\"activation\"]==\"relu\"):\n",
    "            aval=self.relu(z)\n",
    "            self.a[iterno][batch].append(aval)       \n",
    "        \n",
    "        for i in range(2,len(self.layers)):\n",
    "            #print(self.weights[i-1].shape,self.a[iterno][batch][i-1].shape,self.bias[i-1].shape)\n",
    "            z=(self.weights[i-1]*self.a[iterno][batch][i-1])+self.bias[i-1][:,np.arange(0,self.size)]\n",
    "            self.z[iterno][batch].append(z)\n",
    "            #print(z)\n",
    "            if(self.layers[1].getparams()[\"activation\"]==\"sigmoid\"):\n",
    "                aval=self.sigmoid(z)\n",
    "                self.a[iterno][batch].append(aval)\n",
    "            elif(self.layers[1].getparams()[\"activation\"]==\"tanh\"):\n",
    "                aval=self.tanh(z)\n",
    "                self.a[iterno][batch].append(aval)\n",
    "            elif(self.layers[1].getparams()[\"activation\"]==\"relu\"):\n",
    "                aval=self.relu(z)\n",
    "                self.a[iterno][batch].append(aval)\n",
    "        \n",
    "        cost=self.costfunc(m,traindata,trainoutcome,self.a[iterno][batch][i])   \n",
    "        returnval = {\"val\":self.a[iterno][batch][len(self.layers)-1],\"cost\":cost}\n",
    "            \n",
    "        return returnval\n",
    "     \n",
    "    def costfunc(self,m,traindata,trainoutcome,res):\n",
    "        #print(1-res)\n",
    "        A1=np.multiply(trainoutcome,np.log(res))\n",
    "        A2=np.multiply((1-trainoutcome),np.log((1-res)+0.99999))\n",
    "        cost=A1+A2\n",
    "        cost=np.sum(cost)\n",
    "        cost=(-1/m)*cost\n",
    "        return cost\n",
    "    \n",
    "    def backwardProp(self,m,trainoutcome,iterno,batch,hyperparams):\n",
    "        self.optimizationinitialize()\n",
    "        AL=self.a[iterno][batch][len(self.layers)-1]\n",
    "        dAL= - ( np.divide(trainoutcome,AL) - np.divide(1-trainoutcome,1-AL) )\n",
    "        self.da[iterno][batch].append(dAL);\n",
    "        for i in range(0,len(self.layers)-1): \n",
    "            \n",
    "            if(self.layers[1].getparams()[\"activation\"]==\"sigmoid\"):\n",
    "                prime=self.sigmoid_Prime(self.z[iterno][batch][len(self.layers)-2-i])\n",
    "            elif(self.layers[1].getparams()[\"activation\"]==\"tanh\"):\n",
    "                prime=self.tanh_Prime(self.z[iterno][batch][len(self.layers)-2-i])\n",
    "            elif(self.layers[1].getparams()[\"activation\"]==\"relu\"):\n",
    "                prime=self.relu_Prime(self.z[iterno][batch][len(self.layers)-2-i])\n",
    "                \n",
    "            dz = np.multiply(dAL,prime)\n",
    "            self.dz[iterno][batch].append(dz)\n",
    "            dw = (1/self.size) * ( dz * ( self.a[iterno][batch][len(self.layers)-2-i].transpose() ) )\n",
    "            self.dw[iterno][batch].append(dw)\n",
    "            db = (1/self.size)*np.sum(dz,axis=1)\n",
    "            self.db[iterno][batch].append(db)\n",
    "            dAL=(self.weights[len(self.layers)-2-i].transpose())*dz\n",
    "            self.da[iterno][batch].append(dAL)\n",
    "            \n",
    "            # Updating weights\n",
    "            \n",
    "            if(hyperparams[\"optimizer\"]==\"momentum\"):\n",
    "                self.updateMomentum(hyperparams[\"beta\"],hyperparams[\"LearningRate\"],dw,db,i)\n",
    "            elif(hyperparams[\"optimizer\"]==\"adam\"):\n",
    "                self.updateAdam(hyperparams[\"beta1\"],hyperparams[\"beta2\"],hyperparams[\"LearningRate\"],dw,db,i,batch+1,hyperparams[\"epsilon\"])\n",
    "            elif(hyperparams[\"optimizer\"]==\"gd\"):\n",
    "                self.updateGD(hyperparams[\"LearningRate\"],dw,db,i)\n",
    "            \n",
    "    def optimizationinitialize(self):\n",
    "        for i in range(0,len(self.layers)-1):\n",
    "            self.vw.append(np.zeros(list(self.weights[i].shape)))\n",
    "            self.vb.append(np.zeros(list(self.bias[i].shape)))\n",
    "            self.sw.append(np.zeros(list(self.weights[i].shape)))\n",
    "            self.sb.append(np.zeros(list(self.bias[i].shape)))\n",
    "            self.vwcorrected.append(np.zeros(list(self.weights[i].shape)))\n",
    "            self.vbcorrected.append(np.zeros(list(self.bias[i].shape)))\n",
    "            self.swcorrected.append(np.zeros(list(self.weights[i].shape)))\n",
    "            self.sbcorrected.append(np.zeros(list(self.bias[i].shape)))\n",
    "            self.dwforgrad.append(np.zeros(list(self.bias[i].shape)))\n",
    "            self.dbforgrad.append(np.zeros(list(self.bias[i].shape)))\n",
    "            \n",
    "        \n",
    "    def updateGD(self,learningRate,dw,db,i):\n",
    "        self.dwforgrad[len(self.layers)-2-i] = dw\n",
    "        self.dbforgrad[len(self.layers)-2-i] = db\n",
    "        \n",
    "        self.weights[len(self.layers)-2-i] = self.weights[len(self.layers)-2-i] - learningRate*dw\n",
    "        self.bias[len(self.layers)-2-i] = self.bias[len(self.layers)-2-i] - learningRate*db\n",
    "        \n",
    "    def updateMomentum(self,beta,learningRate,dw,db,i):\n",
    "        self.dwforgrad[len(self.layers)-2-i] = dw\n",
    "        self.dbforgrad[len(self.layers)-2-i] = db\n",
    "        \n",
    "        self.vw[len(self.layers)-2-i] = beta*self.vw[len(self.layers)-2-i] + (1-beta)*dw\n",
    "        self.vb[len(self.layers)-2-i] = beta*self.vb[len(self.layers)-2-i] + (1-beta)*db\n",
    "        \n",
    "        self.weights[len(self.layers)-2-i] = self.weights[len(self.layers)-2-i] - learningRate*self.vw[len(self.layers)-2-i]\n",
    "        self.bias[len(self.layers)-2-i] = self.bias[len(self.layers)-2-i] - learningRate*self.vb[len(self.layers)-2-i]\n",
    "    \n",
    "    def updateAdam(self,beta1,beta2,learningRate,dw,db,i,batch,epsilon):\n",
    "        self.dwforgrad[len(self.layers)-2-i] = dw\n",
    "        self.dbforgrad[len(self.layers)-2-i] = db\n",
    "        \n",
    "        self.vw[len(self.layers)-2-i] = beta1*self.vw[len(self.layers)-2-i] + (1-beta1)*dw\n",
    "        self.vb[len(self.layers)-2-i] = beta1*self.vb[len(self.layers)-2-i] + (1-beta1)*db\n",
    "        \n",
    "        vw_corrected = self.vw[len(self.layers)-2-i] / (1 - np.power(beta1,batch) )\n",
    "        vb_corrected = self.vb[len(self.layers)-2-i] / (1 - np.power(beta1,batch) )\n",
    "        \n",
    "        self.vwcorrected[len(self.layers)-2-i]=vw_corrected\n",
    "        self.vbcorrected[len(self.layers)-2-i]=vb_corrected\n",
    "        \n",
    "        self.sw[len(self.layers)-2-i] = beta2*self.sw[len(self.layers)-2-i] + (1-beta2)*np.power(dw,2)\n",
    "        self.sb[len(self.layers)-2-i] = beta2*self.sb[len(self.layers)-2-i] + (1-beta2)*np.power(db,2)\n",
    "        \n",
    "        sw_corrected = self.sw[len(self.layers)-2-i] / (1 - np.power(beta2,batch) )\n",
    "        sb_corrected = self.sb[len(self.layers)-2-i] / (1 - np.power(beta2,batch) )\n",
    "        \n",
    "        self.swcorrected[len(self.layers)-2-i]=sw_corrected\n",
    "        self.sbcorrected[len(self.layers)-2-i]=sb_corrected\n",
    "        \n",
    "        self.weights[len(self.layers)-2-i] = self.weights[len(self.layers)-2-i] - learningRate * (vw_corrected / np.sqrt(sw_corrected + epsilon))\n",
    "        self.bias[len(self.layers)-2-i] = self.bias[len(self.layers)-2-i] - learningRate * (vb_corrected / np.sqrt(sb_corrected + epsilon))\n",
    "        \n",
    "    \n",
    "    def make_mini_batches(self,m,training_data,training_class_names,batch_Size):\n",
    "        mini_batches=[]\n",
    "        new_training_data=training_data\n",
    "        new_training_class_names=training_class_names\n",
    "        \n",
    "        # Randomize The Training Data\n",
    "        permutation = list(np.random.permutation(m))\n",
    "        new_training_data = training_data[:,permutation]\n",
    "        new_training_class_names=training_class_names[:, permutation].reshape((1,m))\n",
    "        \n",
    "        # Get Number Of Batches\n",
    "        num_minibatches = math.floor(m/batch_Size)\n",
    "        # Split The Training_Data Into Batches\n",
    "        for i in range(0,num_minibatches):\n",
    "            training_batch = new_training_data[:,batch_Size*i:batch_Size *(i+1)]\n",
    "            training_class_batch = new_training_class_names[:,batch_Size*i:batch_Size *(i+1)]\n",
    "            mini_batch = (training_batch, training_class_batch)\n",
    "            mini_batches.append(mini_batch)\n",
    "            \n",
    "        # Last Batch If Exists\n",
    "        if m % batch_Size != 0:\n",
    "            training_batch = new_training_data[:,batch_Size*(i+1):m]\n",
    "            training_class_batch = new_training_class_names[:,batch_Size*(i+1):m]\n",
    "            mini_batch = (training_batch, training_class_batch)\n",
    "            mini_batches.append(mini_batch)\n",
    "        \n",
    "        mini_batches = np.asarray(mini_batches)\n",
    "            \n",
    "        return mini_batches\n",
    "        \n",
    "    def sigmoid_Prime(self,value):\n",
    "        part1=self.sigmoid(value)\n",
    "        part2=1-(self.sigmoid(value))\n",
    "        return np.multiply( part1 , part2 )\n",
    "    \n",
    "    def tanh_Prime(self,value):\n",
    "        return 1-(np.power(self.tanh(value),2))\n",
    "    \n",
    "    def relu_Prime(self,value):\n",
    "        return value>=0\n",
    "        \n",
    "    def results(self):\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(self.epochs.T,self.accuracy.T)\n",
    "        plt.title(\"Training Accuracy\")\n",
    "        plt.xlabel(\"epochs\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(self.epochs.T,self.loss.T)\n",
    "        plt.title(\"Training Loss\")\n",
    "        plt.xlabel(\"epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        print(\"Accuracy : {}\".format(self.accuracy[0][len(self.accuracy)-1]))\n",
    "        print(\"loss : {}\".format(self.loss[0][len(self.loss)-1]))\n",
    "    \n",
    "    def sigmoid(self,X):\n",
    "        return 1/(1+np.exp(-X));\n",
    "\n",
    "    def tanh(self,X):\n",
    "        return np.tanh(X);\n",
    "\n",
    "    def relu(self,X):\n",
    "        return np.maximum(0, X);\n",
    "\n",
    "    def getActivation(self):\n",
    "        # plot the activation function using matplotlib library\n",
    "        cur_axes = plt.gca()\n",
    "        # to remove the x axis\n",
    "        cur_axes.axes.get_xaxis().set_visible(False)\n",
    "        rangex=np.linspace(-10, 10, 100)\n",
    "\n",
    "        if(self.activation==\"sigmoid\"):\n",
    "            plt.plot(rangex,self.sigmoid(rangex))\n",
    "        elif(self.activation==\"tanh\"):\n",
    "            plt.plot(rangex,self.tanh(rangex))\n",
    "        elif(self.activation==\"relu\"):\n",
    "            plt.plot(rangex,self.relu(rangex))\n",
    "            plt.title(\"activation function = \"+self.activation)\n",
    "        \n",
    "    class Layers:\n",
    "        \n",
    "        class InputLayer:\n",
    "            \n",
    "            def __init__(self,shape):\n",
    "                self.shape=shape\n",
    "                \n",
    "            def getshape(self):\n",
    "                return np.asarray(self.shape).transpose()\n",
    "        \n",
    "        class DenseLayer:\n",
    "            \n",
    "            def __init__(self,noOfUnits,activation):\n",
    "                self.params={\"activation\":activation,\"shape\":(noOfUnits,1),\"noOfUnits\":noOfUnits}\n",
    "                \n",
    "            def getparams(self):\n",
    "                return self.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 1 has bias shape as (15, 568) and weights shape as (15, 19)\n",
      "layer 2 has bias shape as (10, 568) and weights shape as (10, 15)\n",
      "layer 3 has bias shape as (5, 568) and weights shape as (5, 10)\n",
      "layer 4 has bias shape as (4, 568) and weights shape as (4, 5)\n",
      "layer 5 has bias shape as (1, 568) and weights shape as (1, 4)\n"
     ]
    }
   ],
   "source": [
    "my_brain = Net()\n",
    "l1=Net.Layers.InputLayer(training_data.shape)\n",
    "l2=Net.Layers.DenseLayer(15 , activation=\"tanh\")\n",
    "l3=Net.Layers.DenseLayer(10 , activation=\"tanh\")\n",
    "l4=Net.Layers.DenseLayer(5 , activation=\"tanh\")\n",
    "l5=Net.Layers.DenseLayer(4 , activation=\"tanh\")\n",
    "l6=Net.Layers.DenseLayer(1 , activation=\"sigmoid\")\n",
    "archetecture = my_brain.model(\n",
    "    layers=[l1,l2,l3,l4,l5,l6]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings={\"LearningRate\":0.001,\"Batch_size\":30,\"optimizer\":\"adam\",\"beta1\":0.9,\"beta2\":0.999,\"epochs\":20,\"epsilon\": 1e-8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost for epoch 0 is 0.17472878865167815\n",
      "cost for epoch 1 is 0.1611409388310819\n",
      "cost for epoch 2 is 0.1531234447751384\n",
      "cost for epoch 3 is 0.14844413563008815\n",
      "cost for epoch 4 is 0.14547595350728387\n",
      "cost for epoch 5 is 0.1434518943339061\n",
      "cost for epoch 6 is 0.14199697791470567\n",
      "cost for epoch 7 is 0.14091074242042975\n",
      "cost for epoch 8 is 0.1400765518158546\n",
      "cost for epoch 9 is 0.13942178456299187\n",
      "cost for epoch 10 is 0.1388987993866944\n",
      "cost for epoch 11 is 0.13847504488863357\n",
      "cost for epoch 12 is 0.13812754610134792\n",
      "cost for epoch 13 is 0.1378396509843022\n",
      "cost for epoch 14 is 0.13759901996469637\n",
      "cost for epoch 15 is 0.13739633560563416\n",
      "cost for epoch 16 is 0.1372244477448951\n",
      "cost for epoch 17 is 0.13707779162222475\n",
      "cost for epoch 18 is 0.13695198247300508\n",
      "cost for epoch 19 is 0.13684352726126897\n"
     ]
    }
   ],
   "source": [
    "my_brain.train(training_data,training_class_name,settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = my_brain.make_mini_batches(568,training_data,training_class_name,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(result[18][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 464.0\n",
      "loss : -104.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results=my_brain.results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4  5]\n",
      " [ 4  5  6  7  8]\n",
      " [ 7  8  9  1  2]\n",
      " [11 21 31 41 15]\n",
      " [ 4  5  6  7  8]\n",
      " [ 7  8  9  1  2]\n",
      " [12 22 32 42 52]\n",
      " [ 4  5  6  7  8]\n",
      " [ 7  8  9  1  2]]\n"
     ]
    }
   ],
   "source": [
    "a=np.array([[1,2,3,4,5],[4,5,6,7,8],[7,8,9,1,2],[11,21,31,41,15],[4,5,6,7,8],[7,8,9,1,2],[12,22,32,42,52],[4,5,6,7,8],[7,8,9,1,2]])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 5, 6, 8, 2, 0, 1, 7, 3]\n",
      "[[ 4  5  6  7  8]\n",
      " [ 7  8  9  1  2]\n",
      " [12 22 32 42 52]\n",
      " [ 7  8  9  1  2]\n",
      " [ 7  8  9  1  2]\n",
      " [ 1  2  3  4  5]\n",
      " [ 4  5  6  7  8]\n",
      " [ 4  5  6  7  8]\n",
      " [11 21 31 41 15]]\n"
     ]
    }
   ],
   "source": [
    "permutation = list(np.random.permutation(len(a)))\n",
    "print(permutation)\n",
    "new_a = a[permutation,:]\n",
    "print(new_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc=[[[[0],[1],[2]]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(abc[0][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(0,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.floor(560/30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0000050000287824e-05\n"
     ]
    }
   ],
   "source": [
    "print(np.log(0.99999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "abcd=np.zeros([1,10])\n",
    "print(abcd[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7918088737201365"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "464/586"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
