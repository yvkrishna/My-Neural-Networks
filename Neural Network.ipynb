{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = csv.reader(open(\"hepatitis_2_csv.csv\", \"r\"))\n",
    "x = list(reader)\n",
    "res = np.array(x).astype(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "568\n",
      "(19, 568)\n"
     ]
    }
   ],
   "source": [
    "training_class_name=res[0:568,19:20];\n",
    "training_data=res[0:568,0:19];\n",
    "training_data=np.matrix(training_data)\n",
    "training_data=training_data.transpose()\n",
    "training_class_name=np.matrix(training_class_name)\n",
    "training_class_name=training_class_name.transpose()\n",
    "print(np.prod(list(training_class_name[0].shape)))\n",
    "print(training_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanData=training_data.mean()\n",
    "varianceData=np.var(training_data)\n",
    "training_data=(training_data-meanData)/varianceData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net:\n",
    "    def __init__(self):\n",
    "        self.weights=[];\n",
    "        self.bias=[];\n",
    "        self.status=[];\n",
    "        self.a=[[[]]];\n",
    "        self.z=[[[]]];\n",
    "        self.da=[[[]]];\n",
    "        self.dz=[[[]]];\n",
    "        self.dw=[[[]]];\n",
    "        self.db=[[[]]];\n",
    "        self.cost=[];\n",
    "        self.trainingOutput=[];\n",
    "        self.correctOutput=[];\n",
    "        self.size=0;\n",
    "        self.vw=[];\n",
    "        self.vb=[];\n",
    "        self.sw=[];\n",
    "        self.sb=[];\n",
    "        self.vwcorrected=[];\n",
    "        self.vbcorrected=[];\n",
    "        self.swcorrected=[];\n",
    "        self.sbcorrected=[];\n",
    "        \n",
    "        \n",
    "    def model(self,layers):\n",
    "        self.layers=layers\n",
    "        self.status.append(\"Model archetecture recieved\")\n",
    "        self.createBrain()\n",
    "        self.status.append(\"Model archetecture created\")\n",
    "        return self.layers\n",
    "    \n",
    "    def createBrain(self):\n",
    "        inputshape = self.layers[0].getshape()\n",
    "        self.weights.append(np.random.rand(self.layers[1].getparams()[\"noOfUnits\"],inputshape[0]))\n",
    "        self.bias.append(np.ones([self.layers[1].getparams()[\"noOfUnits\"],inputshape[1]]))\n",
    "        print(\"layer 1 has bias shape as {} and weights shape as {}\".format(self.bias[0].shape,self.weights[0].shape))\n",
    "        for i in range(2,len(self.layers)):\n",
    "            lowerlimit=self.layers[i].getparams()[\"noOfUnits\"]\n",
    "            upperlimit=self.layers[i-1].getparams()[\"noOfUnits\"]\n",
    "            self.weights.append(np.random.rand(lowerlimit,upperlimit))\n",
    "            self.bias.append(np.ones([lowerlimit,inputshape[1]]))\n",
    "            print(\"layer {} has bias shape as {} and weights shape as {}\".format(i,self.bias[i-1].shape,self.weights[i-1].shape))\n",
    "    \n",
    "    def train(self,traindata,trainoutcome,hyperparams):\n",
    "        \n",
    "        numberOfExamples = np.prod(list(training_class_name[0].shape))   \n",
    "        epochs=hyperparams[\"epochs\"]\n",
    "        self.epochs=np.asmatrix(np.arange(0,epochs,1))\n",
    "        democost=0\n",
    "        \n",
    "        if(hyperparams[\"Batch_size\"]>0):\n",
    "            minibatch=True\n",
    "            size=hyperparams[\"Batch_size\"]\n",
    "        \n",
    "            if(minibatch):\n",
    "                # change here\n",
    "                size=30\n",
    "                self.bias[0]=np.ones([self.layers[1].getparams()[\"noOfUnits\"],size])\n",
    "                for i in range(1,len(self.layers)):\n",
    "                    lowerlimit=self.layers[i].getparams()[\"noOfUnits\"]\n",
    "                    self.bias[i-1]=np.ones([lowerlimit,size])\n",
    "\n",
    "                minibatches = self.make_mini_batches(numberOfExamples,traindata,trainoutcome,30)\n",
    "        else:\n",
    "            minibatches = [[(traindata),(trainoutcome)]]\n",
    "            \n",
    "        for i in range(0,epochs):\n",
    "\n",
    "            \n",
    "            for j in range(0,len(minibatches)):                              \n",
    "                # Select a minibatch\n",
    "                (traindata, trainclass) = minibatches[j]\n",
    "                self.size = traindata.shape[1] \n",
    "                op = self.forewardProp(numberOfExamples,traindata,trainclass,i,j)\n",
    "                val = op[\"val\"]\n",
    "                               \n",
    "                if(self.layers[1].getparams()[\"activation\"]==\"sigmoid\"):\n",
    "                    val=val>=0.499\n",
    "                elif(self.layers[1].getparams()[\"activation\"]==\"tanh\" or self.layers[1].getparams()[\"activation\"]==\"relu\"):\n",
    "                    val=val>=0\n",
    "                 \n",
    "                democost=democost+op[\"cost\"]\n",
    "                \n",
    "                self.trainingOutput.append(val)\n",
    "                self.correctOutput.append(trainoutcome)\n",
    "                self.status.append(\"Foreward Propogation for batch {} in epoch {} is completed\".format(j,i))\n",
    "                self.backwardProp(numberOfExamples,trainclass,i,j,hyperparams)\n",
    "                self.status.append(\"Backward Propogation for batch {} in epoch {} is completed\".format(j,i))\n",
    "                self.da[i].append([])\n",
    "                self.a[i].append([])\n",
    "                self.z[i].append([])\n",
    "                self.dz[i].append([])\n",
    "                self.dw[i].append([])\n",
    "                self.db[i].append([])\n",
    "                \n",
    "            self.cost.append(democost)    \n",
    "            print(\"cost for epoch {} is {}\".format(i,democost))   \n",
    "            self.status.append(\"Foreward Propogation for epoch {} is completed\".format(i))\n",
    "            self.status.append(\"Backward Propogation for epoch {} is completed\".format(i))                  \n",
    "            self.da.append([[]])\n",
    "            self.dz.append([[]])\n",
    "            self.dw.append([[]])\n",
    "            self.db.append([[]])\n",
    "            self.a.append([[]])\n",
    "            self.z.append([[]])\n",
    "            \n",
    "        #self.trainingOutput=np.asarray(self.trainingOutput)\n",
    "        #self.correctOutput=np.asarray(self.correctOutput)\n",
    "        #self.dz=np.asarray(self.dz)\n",
    "        #self.dw=np.asarray(self.dw)\n",
    "        #self.db=np.asarray(self.db)\n",
    "        #self.da=np.asarray(self.da)\n",
    "\n",
    "    \n",
    "    def forewardProp(self,m,traindata,trainoutcome,iterno,batch):\n",
    "        self.a[iterno][batch].append(traindata)\n",
    "        \n",
    "        z=self.weights[0]*traindata+self.bias[0][:,np.arange(0,self.size)]\n",
    "        self.z[iterno][batch].append(z)\n",
    "        \n",
    "        if(self.layers[1].getparams()[\"activation\"]==\"sigmoid\"):\n",
    "            aval=self.sigmoid(z)\n",
    "            self.a[iterno][batch].append(aval)\n",
    "        elif(self.layers[1].getparams()[\"activation\"]==\"tanh\"):\n",
    "            aval=self.tanh(z)\n",
    "            self.a[iterno][batch].append(aval)\n",
    "        elif(self.layers[1].getparams()[\"activation\"]==\"relu\"):\n",
    "            aval=self.relu(z)\n",
    "            self.a[iterno][batch].append(aval)\n",
    "        #print(aval.shape)\n",
    "        cost=self.costfunc(m,traindata,trainoutcome,aval)\n",
    "        cost=np.sum(cost)\n",
    "        \n",
    "        \n",
    "        for i in range(2,len(self.layers)):\n",
    "            #print(self.weights[i-1].shape,self.a[iterno][batch][i-1].shape,self.bias[i-1].shape)\n",
    "            z=(self.weights[i-1]*self.a[iterno][batch][i-1])+self.bias[i-1][:,np.arange(0,self.size)]\n",
    "            self.z[iterno][batch].append(z)\n",
    "            if(self.layers[1].getparams()[\"activation\"]==\"sigmoid\"):\n",
    "                aval=self.sigmoid(z)\n",
    "                self.a[iterno][batch].append(aval)\n",
    "            elif(self.layers[1].getparams()[\"activation\"]==\"tanh\"):\n",
    "                aval=self.tanh(z)\n",
    "                self.a[iterno][batch].append(aval)\n",
    "            elif(self.layers[1].getparams()[\"activation\"]==\"relu\"):\n",
    "                aval=self.relu(z)\n",
    "                self.a[iterno][batch].append(aval)\n",
    "        \n",
    "            cost=cost+np.sum( self.costfunc(m,traindata,trainoutcome,aval) )\n",
    "                               \n",
    "        returnval = {\"val\":self.a[iterno][batch][len(self.layers)-1],\"cost\":cost}\n",
    "            \n",
    "        return returnval\n",
    "     \n",
    "    def costfunc(self,m,traindata,trainoutcome,res):\n",
    "        #print(np.multiply(1-trainoutcome,np.log(1-res)))\n",
    "        A1=np.multiply(trainoutcome,np.log(res))\n",
    "        A2=np.multiply((1-trainoutcome),np.log((1-res)))\n",
    "        cost=A1+A2\n",
    "        cost=np.sum(cost)\n",
    "        cost=(-1/m)*cost\n",
    "        return cost\n",
    "    \n",
    "    def backwardProp(self,m,trainoutcome,iterno,batch,hyperparams):\n",
    "        self.optimizationinitialize()\n",
    "        AL=self.a[iterno][batch][len(self.layers)-1]\n",
    "        dAL= - ( np.divide(trainoutcome,AL) - np.divide(1-trainoutcome,1-AL) )\n",
    "        self.da[iterno][batch].append(dAL);\n",
    "        for i in range(0,len(self.layers)-1): \n",
    "            sigmoidprime=self.sigmoid_Prime(self.z[iterno][batch][len(self.layers)-2-i])\n",
    "            dz = np.multiply(dAL,sigmoidprime)\n",
    "            self.dz[iterno][batch].append(dz)\n",
    "            dw = (1/self.size) * ( dz * ( self.a[iterno][batch][len(self.layers)-2-i].transpose() ) )\n",
    "            self.dw[iterno][batch].append(dw)\n",
    "            dz = np.array(dz)\n",
    "            db = (1/m)*np.sum(dz,axis=1,keepdims=True)\n",
    "            db = np.asmatrix(db)\n",
    "            dz = np.asmatrix(dz)\n",
    "            self.db[iterno][batch].append(db)\n",
    "            dAL=(self.weights[len(self.layers)-2-i].transpose())*dz\n",
    "            self.da[iterno][batch].append(dAL)\n",
    "            # Updating weights\n",
    "            if(hyperparams[\"optimizer\"]==\"momentum\"):\n",
    "                self.updateMomentum(hyperparams[\"beta\"],hyperparams[\"LearningRate\"],dw,db,i)\n",
    "            elif(hyperparams[\"optimizer\"]==\"adam\"):\n",
    "                self.updateAdam(hyperparams[\"beta1\"],hyperparams[\"beta2\"],hyperparams[\"LearningRate\"],dw,db,i,batch,hyperparams[\"epsilon\"])\n",
    "            elif(hyperparams[\"optimizer\"]==\"gd\"):\n",
    "                self.updateWB(hyperparams[\"LearningRate\"],dw,db,i)\n",
    "            #print(\"bias are {}\".format(self.bias[len(self.layers)-2]))\n",
    "            \n",
    "    def optimizationinitialize(self):\n",
    "        for i in range(0,len(self.layers)-1):\n",
    "            self.vw.append(np.zeros(list(self.weights[i].shape)))\n",
    "            self.vb.append(np.zeros(list(self.bias[i].shape)))\n",
    "            self.sw.append(np.zeros(list(self.weights[i].shape)))\n",
    "            self.sb.append(np.zeros(list(self.bias[i].shape)))\n",
    "            self.vwcorrected.append(np.zeros(list(self.weights[i].shape)))\n",
    "            self.vbcorrected.append(np.zeros(list(self.bias[i].shape)))\n",
    "            self.swcorrected.append(np.zeros(list(self.weights[i].shape)))\n",
    "            self.sbcorrected.append(np.zeros(list(self.bias[i].shape)))\n",
    "            \n",
    "        \n",
    "    def updateGD(self,learningRate,dw,db,i):\n",
    "        self.weights[len(self.layers)-2-i] = self.weights[len(self.layers)-2-i] - learningRate*dw\n",
    "        self.bias[len(self.layers)-2-i] = self.bias[len(self.layers)-2-i] - learningRate*db\n",
    "        \n",
    "    def updateMomentum(self,beta,learningRate,dw,db,i):\n",
    "        self.vw[len(self.layers)-2-i] = beta*self.vw[len(self.layers)-2-i] + (1-beta)*dw\n",
    "        self.vb[len(self.layers)-2-i] = beta*self.vb[len(self.layers)-2-i] + (1-beta)*db\n",
    "        \n",
    "        self.weights[len(self.layers)-2-i] = self.weights[len(self.layers)-2-i] - learningRate*self.vw[len(self.layers)-2-i]\n",
    "        self.bias[len(self.layers)-2-i] = self.bias[len(self.layers)-2-i] - learningRate*self.vb[len(self.layers)-2-i]\n",
    "    \n",
    "    def updateAdam(self,beta1,beta2,learningRate,dw,db,i,batch,epsilon):\n",
    "        self.vw[len(self.layers)-2-i] = beta1*self.vw[len(self.layers)-2-i] + (1-beta1)*dw\n",
    "        self.vb[len(self.layers)-2-i] = beta1*self.vb[len(self.layers)-2-i] + (1-beta1)*db\n",
    "        \n",
    "        vw_corrected = self.vw[len(self.layers)-2-i] / (1 - np.power(beta1,batch) )\n",
    "        vb_corrected = self.vb[len(self.layers)-2-i] / (1 - np.power(beta1,batch) )\n",
    "        \n",
    "        self.vwcorrected[len(self.layers)-2-i]=vw_corrected\n",
    "        self.vbcorrected[len(self.layers)-2-i]=vb_corrected\n",
    "        \n",
    "        self.sw[len(self.layers)-2-i] = beta2*self.sw[len(self.layers)-2-i] + (1-beta2)*np.power(dw,2)\n",
    "        self.sb[len(self.layers)-2-i] = beta2*self.sb[len(self.layers)-2-i] + (1-beta2)*np.power(db,2)\n",
    "        \n",
    "        sw_corrected = self.sw[len(self.layers)-2-i] / (1 - np.power(beta2,batch) )\n",
    "        sb_corrected = self.sb[len(self.layers)-2-i] / (1 - np.power(beta2,batch) )\n",
    "        \n",
    "        self.swcorrected[len(self.layers)-2-i]=sw_corrected\n",
    "        self.sbcorrected[len(self.layers)-2-i]=sb_corrected\n",
    "        \n",
    "        self.weights[len(self.layers)-2-i] = self.weights[len(self.layers)-2-i] - learningRate * (vw_corrected / np.sqrt(sw_corrected + epsilon))\n",
    "        self.bias[len(self.layers)-2-i] = self.bias[len(self.layers)-2-i] - learningRate * (vb_corrected / np.sqrt(sb_corrected + epsilon))\n",
    "        \n",
    "    \n",
    "    def make_mini_batches(self,m,training_data,training_class_names,batch_Size):\n",
    "        mini_batches=[]\n",
    "        new_training_data=training_data\n",
    "        new_training_class_names=training_class_names\n",
    "        # Randomize The Training Data\n",
    "        #permutation = list(np.random.permutation(m))\n",
    "        #new_training_data = training_data[:,permutation]\n",
    "        #new_training_class_names=training_class_names[:, permutation].reshape((1,m))\n",
    "        # Get Number Of Batches\n",
    "        num_minibatches = math.floor(m/batch_Size)\n",
    "        # Split The Training_Data Into Batches\n",
    "        for i in range(0,num_minibatches):\n",
    "            training_batch = new_training_data[:,batch_Size*i:batch_Size *(i+1)]\n",
    "            training_class_batch = new_training_class_names[:,batch_Size*i:batch_Size *(i+1)]\n",
    "            mini_batch = (training_batch, training_class_batch)\n",
    "            mini_batches.append(mini_batch)\n",
    "            \n",
    "        # Last Batch If Exists\n",
    "        if m % batch_Size != 0:\n",
    "            training_batch = new_training_data[:,batch_Size*(i+1):m]\n",
    "            training_class_batch = new_training_class_names[:,batch_Size*(i+1):m]\n",
    "            ### END CODE HERE ###\n",
    "            mini_batch = (training_batch, training_class_batch)\n",
    "            mini_batches.append(mini_batch)\n",
    "        \n",
    "        mini_batches = np.asarray(mini_batches)\n",
    "            \n",
    "        return mini_batches\n",
    "        \n",
    "    def sigmoid_Prime(self,value):\n",
    "        part1=self.sigmoid(value)\n",
    "        part2=1-(self.sigmoid(value))\n",
    "        return np.multiply( part1 , part2 )\n",
    "    \n",
    "    def tanh_Prime(self,value):\n",
    "        return 1-(self.tanh(value)*self.tanh(value))\n",
    "    \n",
    "    def relu_Prime(self,value):\n",
    "        return value>=0\n",
    "        \n",
    "    def results(self):\n",
    "        trainingOutput=np.asarray(self.trainingOutput)\n",
    "        correctOutput=np.asarray(self.correctOutput)\n",
    "        print(trainingOutput.shape,correctOutput.shape)\n",
    "        #print(self.correctOutput.shape,self.trainingOutput.shape)\n",
    "        ls=correctOutput - trainingOutput\n",
    "        loss=np.sum(ls,axis=1)\n",
    "        loss=loss/len(loss)\n",
    "        ac=np.sum(correctOutput==trainingOutput,axis=1)\n",
    "        accuracy=ac/len(self.correctOutput)\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(self.epochs.T,accuracy)\n",
    "        plt.title(\"Training Accuracy\")\n",
    "        plt.xlabel(\"epochs\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(self.epochs.T,loss)\n",
    "        plt.title(\"Training Loss\")\n",
    "        plt.xlabel(\"epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        #print(\"Accuracy : {}\".format(accuracy[len(accuracy)-1]))\n",
    "        #print(\"loss : {}\".format(loss[len(loss)-1]))\n",
    "    \n",
    "    def sigmoid(self,X):\n",
    "        return 1/(1+np.exp(-X));\n",
    "\n",
    "    def tanh(self,X):\n",
    "        return np.tanh(X);\n",
    "\n",
    "    def relu(self,X):\n",
    "        return np.maximum(0, X);\n",
    "\n",
    "    def getActivation(self):\n",
    "        # plot the activation function using matplotlib library\n",
    "        cur_axes = plt.gca()\n",
    "        # to remove the x axis\n",
    "        cur_axes.axes.get_xaxis().set_visible(False)\n",
    "        rangex=np.linspace(-10, 10, 100)\n",
    "\n",
    "        if(self.activation==\"sigmoid\"):\n",
    "            plt.plot(rangex,self.sigmoid(rangex))\n",
    "        elif(self.activation==\"tanh\"):\n",
    "            plt.plot(rangex,self.tanh(rangex))\n",
    "        elif(self.activation==\"relu\"):\n",
    "            plt.plot(rangex,self.relu(rangex))\n",
    "            plt.title(\"activation function = \"+self.activation)\n",
    "        \n",
    "    class Layers:\n",
    "        \n",
    "        class InputLayer:\n",
    "            \n",
    "            def __init__(self,shape):\n",
    "                self.shape=shape\n",
    "                \n",
    "            def getshape(self):\n",
    "                return np.asarray(self.shape).transpose()\n",
    "        \n",
    "        class DenseLayer:\n",
    "            \n",
    "            def __init__(self,noOfUnits,activation):\n",
    "                self.params={\"activation\":activation,\"shape\":(noOfUnits,1),\"noOfUnits\":noOfUnits}\n",
    "                \n",
    "            def getparams(self):\n",
    "                return self.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 1 has bias shape as (15, 568) and weights shape as (15, 19)\n",
      "layer 2 has bias shape as (10, 568) and weights shape as (10, 15)\n",
      "layer 3 has bias shape as (5, 568) and weights shape as (5, 10)\n",
      "layer 4 has bias shape as (4, 568) and weights shape as (4, 5)\n",
      "layer 5 has bias shape as (1, 568) and weights shape as (1, 4)\n"
     ]
    }
   ],
   "source": [
    "my_brain = Net()\n",
    "l1=Net.Layers.InputLayer(training_data.shape)\n",
    "l2=Net.Layers.DenseLayer(15 , activation=\"sigmoid\")\n",
    "l3=Net.Layers.DenseLayer(10 , activation=\"sigmoid\")\n",
    "l4=Net.Layers.DenseLayer(5 , activation=\"sigmoid\")\n",
    "l5=Net.Layers.DenseLayer(4 , activation=\"sigmoid\")\n",
    "l6=Net.Layers.DenseLayer(1 , activation=\"sigmoid\")\n",
    "archetecture = my_brain.model(\n",
    "    layers=[l1,l2,l3,l4,l5,l6]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings={\"LearningRate\":0.1,\"Batch_size\":30,\"optimizer\":\"adam\",\"beta1\":0.9,\"beta2\":0.999,\"epochs\":20,\"epsilon\": 1e-8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vedha krishna\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:159: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\vedha krishna\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:159: RuntimeWarning: invalid value encountered in multiply\n",
      "C:\\Users\\vedha krishna\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:219: RuntimeWarning: divide by zero encountered in true_divide\n",
      "C:\\Users\\vedha krishna\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:220: RuntimeWarning: divide by zero encountered in true_divide\n",
      "C:\\Users\\vedha krishna\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:228: RuntimeWarning: divide by zero encountered in true_divide\n",
      "C:\\Users\\vedha krishna\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:229: RuntimeWarning: divide by zero encountered in true_divide\n",
      "C:\\Users\\vedha krishna\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:234: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\vedha krishna\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:235: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\vedha krishna\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:219: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\vedha krishna\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:220: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\vedha krishna\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:228: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\vedha krishna\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:229: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\vedha krishna\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:79: RuntimeWarning: invalid value encountered in greater_equal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost for epoch 0 is nan\n",
      "cost for epoch 1 is nan\n",
      "cost for epoch 2 is nan\n",
      "cost for epoch 3 is nan\n",
      "cost for epoch 4 is nan\n",
      "cost for epoch 5 is nan\n",
      "cost for epoch 6 is nan\n",
      "cost for epoch 7 is nan\n",
      "cost for epoch 8 is nan\n",
      "cost for epoch 9 is nan\n",
      "cost for epoch 10 is nan\n",
      "cost for epoch 11 is nan\n",
      "cost for epoch 12 is nan\n",
      "cost for epoch 13 is nan\n",
      "cost for epoch 14 is nan\n",
      "cost for epoch 15 is nan\n",
      "cost for epoch 16 is nan\n",
      "cost for epoch 17 is nan\n",
      "cost for epoch 18 is nan\n",
      "cost for epoch 19 is nan\n"
     ]
    }
   ],
   "source": [
    "my_brain.train(training_data,training_class_name,settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = my_brain.make_mini_batches(568,training_data,training_class_name,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28)\n"
     ]
    }
   ],
   "source": [
    "print(result[18][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1, 568) (10, 1, 568)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results=my_brain.results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4  5]\n",
      " [ 4  5  6  7  8]\n",
      " [ 7  8  9  1  2]\n",
      " [11 21 31 41 15]\n",
      " [ 4  5  6  7  8]\n",
      " [ 7  8  9  1  2]\n",
      " [12 22 32 42 52]\n",
      " [ 4  5  6  7  8]\n",
      " [ 7  8  9  1  2]]\n"
     ]
    }
   ],
   "source": [
    "a=np.array([[1,2,3,4,5],[4,5,6,7,8],[7,8,9,1,2],[11,21,31,41,15],[4,5,6,7,8],[7,8,9,1,2],[12,22,32,42,52],[4,5,6,7,8],[7,8,9,1,2]])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 6, 0, 4, 5, 1, 2, 7, 3]\n",
      "[[ 7  8  9  1  2]\n",
      " [12 22 32 42 52]\n",
      " [ 1  2  3  4  5]\n",
      " [ 4  5  6  7  8]\n",
      " [ 7  8  9  1  2]\n",
      " [ 4  5  6  7  8]\n",
      " [ 7  8  9  1  2]\n",
      " [ 4  5  6  7  8]\n",
      " [11 21 31 41 15]]\n"
     ]
    }
   ],
   "source": [
    "permutation = list(np.random.permutation(len(a)))\n",
    "print(permutation)\n",
    "new_a = a[permutation,:]\n",
    "print(new_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc=[[[[0],[1],[2]]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(abc[0][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(0,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.0\n"
     ]
    }
   ],
   "source": [
    "print(np.floor(560/30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
