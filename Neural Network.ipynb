{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = csv.reader(open(\"hepatitis_2_csv.csv\", \"r\"))\n",
    "x = list(reader)\n",
    "res = np.array(x).astype(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "568\n",
      "(19, 568)\n"
     ]
    }
   ],
   "source": [
    "training_class_name=res[0:568,19:20];\n",
    "training_data=res[0:568,0:19];\n",
    "training_data=np.matrix(training_data)\n",
    "training_data=training_data.transpose()\n",
    "training_class_name=np.matrix(training_class_name)\n",
    "training_class_name=training_class_name.transpose()\n",
    "print(np.prod(list(training_class_name[0].shape)))\n",
    "print(training_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanData=training_data.mean()\n",
    "varianceData=np.var(training_data)\n",
    "training_data=(training_data-meanData)/varianceData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net:\n",
    "    def __init__(self):\n",
    "        self.weights=[];\n",
    "        self.bias=[];\n",
    "        self.status=[];\n",
    "        self.a=[[[]]];\n",
    "        self.z=[[[]]];\n",
    "        self.da=[[[]]];\n",
    "        self.dz=[[[]]];\n",
    "        self.dw=[[[]]];\n",
    "        self.db=[[[]]];\n",
    "        self.cost=[];\n",
    "        self.trainingOutput=[];\n",
    "        self.correctOutput=[];\n",
    "        self.size=0;\n",
    "        self.vw=[];\n",
    "        self.vb=[];\n",
    "        self.sw=[];\n",
    "        self.sb=[];\n",
    "        self.vwcorrected=[];\n",
    "        self.vbcorrected=[];\n",
    "        self.swcorrected=[];\n",
    "        self.sbcorrected=[];\n",
    "        \n",
    "        \n",
    "    def model(self,layers):\n",
    "        self.layers=layers\n",
    "        self.status.append(\"Model archetecture recieved\")\n",
    "        self.createBrain()\n",
    "        self.status.append(\"Model archetecture created\")\n",
    "        return self.layers\n",
    "    \n",
    "    def createBrain(self):\n",
    "        inputshape = self.layers[0].getshape()\n",
    "        self.weights.append(0.01*np.random.rand(self.layers[1].getparams()[\"noOfUnits\"],inputshape[0]))\n",
    "        self.bias.append(np.ones([self.layers[1].getparams()[\"noOfUnits\"],inputshape[1]]))\n",
    "        print(\"layer 1 has bias shape as {} and weights shape as {}\".format(self.bias[0].shape,self.weights[0].shape))\n",
    "        for i in range(2,len(self.layers)):\n",
    "            lowerlimit=self.layers[i].getparams()[\"noOfUnits\"]\n",
    "            upperlimit=self.layers[i-1].getparams()[\"noOfUnits\"]\n",
    "            self.weights.append(0.01*np.random.rand(lowerlimit,upperlimit))\n",
    "            self.bias.append(np.ones([lowerlimit,inputshape[1]]))\n",
    "            print(\"layer {} has bias shape as {} and weights shape as {}\".format(i,self.bias[i-1].shape,self.weights[i-1].shape))\n",
    "    \n",
    "    def train(self,traindata,trainoutcome,hyperparams):\n",
    "        \n",
    "        numberOfExamples = np.prod(list(training_class_name[0].shape))   \n",
    "        epochs=hyperparams[\"epochs\"]\n",
    "        self.epochs=np.asmatrix(np.arange(0,epochs,1))\n",
    "        democost=0\n",
    "        \n",
    "        if(hyperparams[\"Batch_size\"]>0):\n",
    "            minibatch=True\n",
    "            size=hyperparams[\"Batch_size\"]\n",
    "        \n",
    "            if(minibatch):\n",
    "                self.bias[0]=np.ones([self.layers[1].getparams()[\"noOfUnits\"],size])\n",
    "                for i in range(1,len(self.layers)):\n",
    "                    lowerlimit=self.layers[i].getparams()[\"noOfUnits\"]\n",
    "                    self.bias[i-1]=np.ones([lowerlimit,size])\n",
    "\n",
    "                minibatches = self.make_mini_batches(numberOfExamples,traindata,trainoutcome,30)\n",
    "        else:\n",
    "            minibatches = [[(traindata),(trainoutcome)]]\n",
    "            \n",
    "        for i in range(0,epochs):\n",
    "\n",
    "            \n",
    "            for j in range(0,len(minibatches)):                              \n",
    "                # Select a minibatch\n",
    "                (traindata, trainclass) = minibatches[j]\n",
    "                self.size = traindata.shape[1] \n",
    "                op = self.forewardProp(numberOfExamples,traindata,trainclass,i,j)\n",
    "                val = op[\"val\"]\n",
    "                               \n",
    "                if(self.layers[1].getparams()[\"activation\"]==\"sigmoid\"):\n",
    "                    val=val>=0.499\n",
    "                elif(self.layers[1].getparams()[\"activation\"]==\"tanh\" or self.layers[1].getparams()[\"activation\"]==\"relu\"):\n",
    "                    val=val>=0\n",
    "                 \n",
    "                democost=democost+op[\"cost\"]\n",
    "                \n",
    "                self.trainingOutput.append(val)\n",
    "                self.correctOutput.append(trainoutcome)\n",
    "                self.status.append(\"Foreward Propogation for batch {} in epoch {} is completed\".format(j,i))\n",
    "                self.backwardProp(numberOfExamples,trainclass,i,j,hyperparams)\n",
    "                self.status.append(\"Backward Propogation for batch {} in epoch {} is completed\".format(j,i))\n",
    "                self.da[i].append([])\n",
    "                self.a[i].append([])\n",
    "                self.z[i].append([])\n",
    "                self.dz[i].append([])\n",
    "                self.dw[i].append([])\n",
    "                self.db[i].append([])\n",
    "                \n",
    "            self.cost.append(democost)    \n",
    "            print(\"cost for epoch {} is {}\".format(i,democost))   \n",
    "            self.status.append(\"Foreward Propogation for epoch {} is completed\".format(i))\n",
    "            self.status.append(\"Backward Propogation for epoch {} is completed\".format(i))                  \n",
    "            self.da.append([[]])\n",
    "            self.dz.append([[]])\n",
    "            self.dw.append([[]])\n",
    "            self.db.append([[]])\n",
    "            self.a.append([[]])\n",
    "            self.z.append([[]])\n",
    "            \n",
    "        #self.trainingOutput=np.asarray(self.trainingOutput)\n",
    "        #self.correctOutput=np.asarray(self.correctOutput)\n",
    "        #self.dz=np.asarray(self.dz)\n",
    "        #self.dw=np.asarray(self.dw)\n",
    "        #self.db=np.asarray(self.db)\n",
    "        #self.da=np.asarray(self.da)\n",
    "\n",
    "    \n",
    "    def forewardProp(self,m,traindata,trainoutcome,iterno,batch):\n",
    "        self.a[iterno][batch].append(traindata)\n",
    "        z=self.weights[0]*traindata+self.bias[0][:,np.arange(0,self.size)]\n",
    "        self.z[iterno][batch].append(z)\n",
    "        \n",
    "        if(self.layers[1].getparams()[\"activation\"]==\"sigmoid\"):\n",
    "            aval=self.sigmoid(z)\n",
    "            self.a[iterno][batch].append(aval)\n",
    "        elif(self.layers[1].getparams()[\"activation\"]==\"tanh\"):\n",
    "            aval=self.tanh(z)\n",
    "            self.a[iterno][batch].append(aval)\n",
    "        elif(self.layers[1].getparams()[\"activation\"]==\"relu\"):\n",
    "            aval=self.relu(z)\n",
    "            self.a[iterno][batch].append(aval)\n",
    "        #print(aval.shape)\n",
    "        \n",
    "        \n",
    "        for i in range(2,len(self.layers)):\n",
    "            #print(self.weights[i-1].shape,self.a[iterno][batch][i-1].shape,self.bias[i-1].shape)\n",
    "            z=(self.weights[i-1]*self.a[iterno][batch][i-1])+self.bias[i-1][:,np.arange(0,self.size)]\n",
    "            self.z[iterno][batch].append(z)\n",
    "            #print(z)\n",
    "            if(self.layers[1].getparams()[\"activation\"]==\"sigmoid\"):\n",
    "                aval=self.sigmoid(z)\n",
    "                self.a[iterno][batch].append(aval)\n",
    "            elif(self.layers[1].getparams()[\"activation\"]==\"tanh\"):\n",
    "                aval=self.tanh(z)\n",
    "                self.a[iterno][batch].append(aval)\n",
    "            elif(self.layers[1].getparams()[\"activation\"]==\"relu\"):\n",
    "                aval=self.relu(z)\n",
    "                self.a[iterno][batch].append(aval)\n",
    "            #print(\"self.weights = {} trainingdata = {} bias = {} z = {}\".format(self.weights[i-1].shape,self.a[iterno][batch][i-1].shape,self.bias[i-1].shape,z.shape))\n",
    "        #print(\"a = {}\".format(self.a[iterno][batch][len(self.layers)-1]))\n",
    "        #print(self.z[iterno][batch][len(self.layers)-2])\n",
    "        \n",
    "        cost=self.costfunc(m,traindata,trainoutcome,self.a[iterno][batch][i])                   \n",
    "        returnval = {\"val\":self.a[iterno][batch][len(self.layers)-1],\"cost\":cost}\n",
    "            \n",
    "        return returnval\n",
    "     \n",
    "    def costfunc(self,m,traindata,trainoutcome,res):\n",
    "        #print(np.multiply(1-trainoutcome,np.log(1-res)))\n",
    "        A1=np.multiply(trainoutcome,np.log(res))\n",
    "        A2=np.multiply((1-trainoutcome),np.log((1-res)))\n",
    "        cost=A1+A2\n",
    "        cost=np.sum(cost)\n",
    "        cost=(-1/m)*cost\n",
    "        return cost\n",
    "    \n",
    "    def backwardProp(self,m,trainoutcome,iterno,batch,hyperparams):\n",
    "        self.optimizationinitialize()\n",
    "        AL=self.a[iterno][batch][len(self.layers)-1]\n",
    "        dAL= - ( np.divide(trainoutcome,AL) - np.divide(1-trainoutcome,1-AL) )\n",
    "        self.da[iterno][batch].append(dAL);\n",
    "        for i in range(0,len(self.layers)-1): \n",
    "            sigmoidprime=self.sigmoid_Prime(self.z[iterno][batch][len(self.layers)-2-i])\n",
    "            dz = np.multiply(dAL,sigmoidprime)\n",
    "            self.dz[iterno][batch].append(dz)\n",
    "            dw = (1/self.size) * ( dz * ( self.a[iterno][batch][len(self.layers)-2-i].transpose() ) )\n",
    "            #print(dw)\n",
    "            self.dw[iterno][batch].append(dw)\n",
    "            #dz = np.array(dz)\n",
    "            db = (1/self.size)*np.sum(dz,axis=1)\n",
    "            #print(db)\n",
    "            #db = np.asmatrix(db)\n",
    "            #dz = np.asmatrix(dz)\n",
    "            self.db[iterno][batch].append(db)\n",
    "            dAL=(self.weights[len(self.layers)-2-i].transpose())*dz\n",
    "            self.da[iterno][batch].append(dAL)\n",
    "            # Updating weights\n",
    "            if(hyperparams[\"optimizer\"]==\"momentum\"):\n",
    "                self.updateMomentum(hyperparams[\"beta\"],hyperparams[\"LearningRate\"],dw,db,i)\n",
    "            elif(hyperparams[\"optimizer\"]==\"adam\"):\n",
    "                self.updateAdam(hyperparams[\"beta1\"],hyperparams[\"beta2\"],hyperparams[\"LearningRate\"],dw,db,i,batch+1,hyperparams[\"epsilon\"])\n",
    "            elif(hyperparams[\"optimizer\"]==\"gd\"):\n",
    "                self.updateGD(hyperparams[\"LearningRate\"],dw,db,i)\n",
    "            #print(\"bias are {}\".format(self.bias[len(self.layers)-2]))\n",
    "            \n",
    "    def optimizationinitialize(self):\n",
    "        for i in range(0,len(self.layers)-1):\n",
    "            self.vw.append(np.zeros(list(self.weights[i].shape)))\n",
    "            self.vb.append(np.zeros(list(self.bias[i].shape)))\n",
    "            self.sw.append(np.zeros(list(self.weights[i].shape)))\n",
    "            self.sb.append(np.zeros(list(self.bias[i].shape)))\n",
    "            self.vwcorrected.append(np.zeros(list(self.weights[i].shape)))\n",
    "            self.vbcorrected.append(np.zeros(list(self.bias[i].shape)))\n",
    "            self.swcorrected.append(np.zeros(list(self.weights[i].shape)))\n",
    "            self.sbcorrected.append(np.zeros(list(self.bias[i].shape)))\n",
    "            \n",
    "        \n",
    "    def updateGD(self,learningRate,dw,db,i):\n",
    "        self.weights[len(self.layers)-2-i] = self.weights[len(self.layers)-2-i] - learningRate*dw\n",
    "        self.bias[len(self.layers)-2-i] = self.bias[len(self.layers)-2-i] - learningRate*db\n",
    "        \n",
    "    def updateMomentum(self,beta,learningRate,dw,db,i):\n",
    "        self.vw[len(self.layers)-2-i] = beta*self.vw[len(self.layers)-2-i] + (1-beta)*dw\n",
    "        self.vb[len(self.layers)-2-i] = beta*self.vb[len(self.layers)-2-i] + (1-beta)*db\n",
    "        \n",
    "        self.weights[len(self.layers)-2-i] = self.weights[len(self.layers)-2-i] - learningRate*self.vw[len(self.layers)-2-i]\n",
    "        self.bias[len(self.layers)-2-i] = self.bias[len(self.layers)-2-i] - learningRate*self.vb[len(self.layers)-2-i]\n",
    "    \n",
    "    def updateAdam(self,beta1,beta2,learningRate,dw,db,i,batch,epsilon):\n",
    "        #print(\"beta1 = {} beta2 = {} learningRate = {} dw ={} db = {} i = {} batch ={} epsilon = {}\".format(beta1,beta2,learningRate,dw,db,i,batch,epsilon))\n",
    "       \n",
    "        self.vw[len(self.layers)-2-i] = beta1*self.vw[len(self.layers)-2-i] + (1-beta1)*dw\n",
    "        self.vb[len(self.layers)-2-i] = beta1*self.vb[len(self.layers)-2-i] + (1-beta1)*db\n",
    "        \n",
    "        #print(\"beta = {} batch = {} val = {}\".format(beta1,batch,1 - np.power(beta1,batch)))\n",
    "        \n",
    "        vw_corrected = self.vw[len(self.layers)-2-i] / (1 - np.power(beta1,batch) )\n",
    "        vb_corrected = self.vb[len(self.layers)-2-i] / (1 - np.power(beta1,batch) )\n",
    "        \n",
    "        self.vwcorrected[len(self.layers)-2-i]=vw_corrected\n",
    "        self.vbcorrected[len(self.layers)-2-i]=vb_corrected\n",
    "        \n",
    "        self.sw[len(self.layers)-2-i] = beta2*self.sw[len(self.layers)-2-i] + (1-beta2)*np.power(dw,2)\n",
    "        self.sb[len(self.layers)-2-i] = beta2*self.sb[len(self.layers)-2-i] + (1-beta2)*np.power(db,2)\n",
    "        \n",
    "        sw_corrected = self.sw[len(self.layers)-2-i] / (1 - np.power(beta2,batch) )\n",
    "        sb_corrected = self.sb[len(self.layers)-2-i] / (1 - np.power(beta2,batch) )\n",
    "        \n",
    "        self.swcorrected[len(self.layers)-2-i]=sw_corrected\n",
    "        self.sbcorrected[len(self.layers)-2-i]=sb_corrected\n",
    "        \n",
    "        #print(\"val = \",(vw_corrected / np.sqrt(sw_corrected + epsilon)))\n",
    "        #print()\n",
    "        #print(\"first time  =\",self.weights[len(self.layers)-2-i])\n",
    "        self.weights[len(self.layers)-2-i] = self.weights[len(self.layers)-2-i] - learningRate * (vw_corrected / np.sqrt(sw_corrected + epsilon))\n",
    "        self.bias[len(self.layers)-2-i] = self.bias[len(self.layers)-2-i] - learningRate * (vb_corrected / np.sqrt(sb_corrected + epsilon))\n",
    "        \n",
    "        \n",
    "        #print(\"second time = \",self.weights[len(self.layers)-2-i])\n",
    "        \n",
    "    \n",
    "    def make_mini_batches(self,m,training_data,training_class_names,batch_Size):\n",
    "        mini_batches=[]\n",
    "        new_training_data=training_data\n",
    "        new_training_class_names=training_class_names\n",
    "        # Randomize The Training Data\n",
    "        #permutation = list(np.random.permutation(m))\n",
    "        #new_training_data = training_data[:,permutation]\n",
    "        #new_training_class_names=training_class_names[:, permutation].reshape((1,m))\n",
    "        # Get Number Of Batches\n",
    "        num_minibatches = math.floor(m/batch_Size)\n",
    "        # Split The Training_Data Into Batches\n",
    "        for i in range(0,num_minibatches):\n",
    "            training_batch = new_training_data[:,batch_Size*i:batch_Size *(i+1)]\n",
    "            training_class_batch = new_training_class_names[:,batch_Size*i:batch_Size *(i+1)]\n",
    "            mini_batch = (training_batch, training_class_batch)\n",
    "            mini_batches.append(mini_batch)\n",
    "            \n",
    "        # Last Batch If Exists\n",
    "        if m % batch_Size != 0:\n",
    "            training_batch = new_training_data[:,batch_Size*(i+1):m]\n",
    "            training_class_batch = new_training_class_names[:,batch_Size*(i+1):m]\n",
    "            ### END CODE HERE ###\n",
    "            mini_batch = (training_batch, training_class_batch)\n",
    "            mini_batches.append(mini_batch)\n",
    "        \n",
    "        mini_batches = np.asarray(mini_batches)\n",
    "            \n",
    "        return mini_batches\n",
    "        \n",
    "    def sigmoid_Prime(self,value):\n",
    "        part1=self.sigmoid(value)\n",
    "        part2=1-(self.sigmoid(value))\n",
    "        return np.multiply( part1 , part2 )\n",
    "    \n",
    "    def tanh_Prime(self,value):\n",
    "        return 1-(self.tanh(value)*self.tanh(value))\n",
    "    \n",
    "    def relu_Prime(self,value):\n",
    "        return value>=0\n",
    "        \n",
    "    def results(self):\n",
    "        trainingOutput=np.asarray(self.trainingOutput)\n",
    "        correctOutput=np.asarray(self.correctOutput)\n",
    "        print(trainingOutput.shape,correctOutput.shape)\n",
    "        #print(self.correctOutput.shape,self.trainingOutput.shape)\n",
    "        ls=correctOutput - trainingOutput\n",
    "        loss=np.sum(ls,axis=1)\n",
    "        loss=loss/len(loss)\n",
    "        ac=np.sum(correctOutput==trainingOutput,axis=1)\n",
    "        accuracy=ac/len(self.correctOutput)\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(self.epochs.T,accuracy)\n",
    "        plt.title(\"Training Accuracy\")\n",
    "        plt.xlabel(\"epochs\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(self.epochs.T,loss)\n",
    "        plt.title(\"Training Loss\")\n",
    "        plt.xlabel(\"epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        #print(\"Accuracy : {}\".format(accuracy[len(accuracy)-1]))\n",
    "        #print(\"loss : {}\".format(loss[len(loss)-1]))\n",
    "    \n",
    "    def sigmoid(self,X):\n",
    "        return 1/(1+np.exp(-X));\n",
    "\n",
    "    def tanh(self,X):\n",
    "        return np.tanh(X);\n",
    "\n",
    "    def relu(self,X):\n",
    "        return np.maximum(0, X);\n",
    "\n",
    "    def getActivation(self):\n",
    "        # plot the activation function using matplotlib library\n",
    "        cur_axes = plt.gca()\n",
    "        # to remove the x axis\n",
    "        cur_axes.axes.get_xaxis().set_visible(False)\n",
    "        rangex=np.linspace(-10, 10, 100)\n",
    "\n",
    "        if(self.activation==\"sigmoid\"):\n",
    "            plt.plot(rangex,self.sigmoid(rangex))\n",
    "        elif(self.activation==\"tanh\"):\n",
    "            plt.plot(rangex,self.tanh(rangex))\n",
    "        elif(self.activation==\"relu\"):\n",
    "            plt.plot(rangex,self.relu(rangex))\n",
    "            plt.title(\"activation function = \"+self.activation)\n",
    "        \n",
    "    class Layers:\n",
    "        \n",
    "        class InputLayer:\n",
    "            \n",
    "            def __init__(self,shape):\n",
    "                self.shape=shape\n",
    "                \n",
    "            def getshape(self):\n",
    "                return np.asarray(self.shape).transpose()\n",
    "        \n",
    "        class DenseLayer:\n",
    "            \n",
    "            def __init__(self,noOfUnits,activation):\n",
    "                self.params={\"activation\":activation,\"shape\":(noOfUnits,1),\"noOfUnits\":noOfUnits}\n",
    "                \n",
    "            def getparams(self):\n",
    "                return self.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 1 has bias shape as (15, 568) and weights shape as (15, 19)\n",
      "layer 2 has bias shape as (10, 568) and weights shape as (10, 15)\n",
      "layer 3 has bias shape as (5, 568) and weights shape as (5, 10)\n",
      "layer 4 has bias shape as (4, 568) and weights shape as (4, 5)\n",
      "layer 5 has bias shape as (1, 568) and weights shape as (1, 4)\n"
     ]
    }
   ],
   "source": [
    "my_brain = Net()\n",
    "l1=Net.Layers.InputLayer(training_data.shape)\n",
    "l2=Net.Layers.DenseLayer(15 , activation=\"sigmoid\")\n",
    "l3=Net.Layers.DenseLayer(10 , activation=\"sigmoid\")\n",
    "l4=Net.Layers.DenseLayer(5 , activation=\"sigmoid\")\n",
    "l5=Net.Layers.DenseLayer(4 , activation=\"sigmoid\")\n",
    "l6=Net.Layers.DenseLayer(1 , activation=\"sigmoid\")\n",
    "archetecture = my_brain.model(\n",
    "    layers=[l1,l2,l3,l4,l5,l6]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings={\"LearningRate\":0.001,\"Batch_size\":30,\"optimizer\":\"adam\",\"beta1\":0.9,\"beta2\":0.999,\"epochs\":10,\"epsilon\": 1e-8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost for epoch 0 is 0.49437735844637504\n",
      "cost for epoch 1 is 0.9883672818439576\n",
      "cost for epoch 2 is 1.481978648014717\n",
      "cost for epoch 3 is 1.9752201113968277\n",
      "cost for epoch 4 is 2.4681001092772075\n",
      "cost for epoch 5 is 2.96062686783112\n",
      "cost for epoch 6 is 3.4528084079754238\n",
      "cost for epoch 7 is 3.94465255104178\n",
      "cost for epoch 8 is 4.43616692427591\n",
      "cost for epoch 9 is 4.927358966168791\n"
     ]
    }
   ],
   "source": [
    "my_brain.train(training_data,training_class_name,settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = my_brain.make_mini_batches(568,training_data,training_class_name,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28)\n"
     ]
    }
   ],
   "source": [
    "print(result[18][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (30) into shape (1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-298-d4a30ca99905>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresults\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmy_brain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-292-314b3523c9fe>\u001b[0m in \u001b[0;36mresults\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m         \u001b[0mtrainingOutput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainingOutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m         \u001b[0mcorrectOutput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorrectOutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingOutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcorrectOutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m     \"\"\"\n\u001b[1;32m--> 538\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (30) into shape (1)"
     ]
    }
   ],
   "source": [
    "results=my_brain.results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.array([[1,2,3,4,5],[4,5,6,7,8],[7,8,9,1,2],[11,21,31,41,15],[4,5,6,7,8],[7,8,9,1,2],[12,22,32,42,52],[4,5,6,7,8],[7,8,9,1,2]])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation = list(np.random.permutation(len(a)))\n",
    "print(permutation)\n",
    "new_a = a[permutation,:]\n",
    "print(new_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc=[[[[0],[1],[2]]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(abc[0][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(0,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.floor(560/30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
